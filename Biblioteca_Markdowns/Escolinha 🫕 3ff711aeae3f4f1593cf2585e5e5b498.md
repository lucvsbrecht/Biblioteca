# Escolinha  ü´ï

# **Python**

## **B√°sico**

### **Listas**

# Lists in Python

Listas s√£o uma das estruturas de dados mais comumente usadas em Python, e elas nos permitem armazenar uma cole√ß√£o de itens em uma √∫nica vari√°vel.

## Criando uma lista

Para criar uma lista em Python, usamos colchetes e separamos os itens com v√≠rgulas. Por exemplo:

```python
frutas = ['ma√ß√£', 'banana', 'laranja']

```

## Acessando itens da lista

Podemos acessar itens individuais em uma lista usando seu √≠ndice. O primeiro item em uma lista tem um √≠ndice de 0, o segundo item tem um √≠ndice de 1 e assim por diante. Por exemplo:

```python
frutas = ['ma√ß√£', 'banana', 'laranja']
print(frutas[0])  # Sa√≠da: 'ma√ß√£'
print(frutas[1])  # Sa√≠da: 'banana'
print(frutas[2])  # Sa√≠da: 'laranja'

```

Tamb√©m podemos acessar itens do final da lista usando √≠ndices negativos. O √∫ltimo item em uma lista tem um √≠ndice de -1, o pen√∫ltimo item tem um √≠ndice de -2 e assim por diante. Por exemplo:

```python
frutas = ['ma√ß√£', 'banana', 'laranja']
print(frutas[-1])  # Sa√≠da: 'laranja'
print(frutas[-2])  # Sa√≠da: 'banana'
print(frutas[-3])  # Sa√≠da: 'ma√ß√£'
```

## Fatiando listas

Tamb√©m podemos extrair um subconjunto de uma lista usando o fatiamento. O fatiamento cria uma nova lista que cont√©m apenas os elementos especificados da lista original. A sintaxe b√°sica para fatiar uma lista √© a seguinte:

```python
nova_lista = lista[in√≠cio:fim]

```

onde o `in√≠cio` √© o √≠ndice do primeiro item a incluir no fatiamento e o `fim` √© o √≠ndice do primeiro item a excluir do fatiamento.
Por exemplo:

```python
numeros = [1, 2, 3, 4, 5]
print(numeros[1:4])  # Sa√≠da: [2, 3, 4]

```

## Modificando itens da lista

Podemos modificar itens individuais em uma lista usando seu √≠ndice. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas[1] = 'uva'
print(frutas)  # Sa√≠da: ['ma√ß√£', 'uva', 'laranja']

```

## Adicionando itens a uma lista

Podemos adicionar itens ao final de uma lista usando o m√©todo `append()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas.append('pera')
print(frutas)  # Sa√≠da: ['ma√ß√£', 'banana', 'laranja', 'pera']

```

Tamb√©m podemos inserir itens em uma posi√ß√£o espec√≠fica da lista usando o m√©todo `insert()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas.insert(1, 'pera')
print(frutas)  # Sa√≠da: ['ma√ß√£', 'pera', 'banana', 'laranja']

```

## Removendo itens de uma lista

Podemos remover itens de uma lista usando o m√©todo `remove()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas.remove('banana')
print(frutas)  # Sa√≠da: ['ma√ß√£', 'laranja']

```

Tamb√©m podemos remover itens do final de uma lista usando o m√©todo `pop()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas.pop()
print(frutas)  # Sa√≠da: ['ma√ß√£', 'banana']

```

## Compreens√µes de lista

As compreens√µes de lista s√£o uma maneira concisa de criar novas listas executando alguma opera√ß√£o em cada item em uma lista existente. A sintaxe b√°sica para uma compreens√£o de lista √© a seguinte:

```
nova_lista = [express√£o for item in lista if condi√ß√£o]

```

Por exemplo:

```
numeros = [1, 2, 3, 4, 5]
pares = [x for x in numeros if x % 2 == 0]
print(pares)  # Sa√≠da: [2, 4]

```

Podemos adicionar itens ao final de uma lista usando o m√©todo `append()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas.append('pera')
print(frutas)  # Sa√≠da: ['ma√ß√£', 'banana', 'laranja', 'pera']

```

Tamb√©m podemos inserir itens em uma posi√ß√£o espec√≠fica da lista usando o m√©todo `insert()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja']
frutas.insert(1, 'pera')
print(frutas)  # Sa√≠da: ['ma√ß√£', 'pera', 'banana', 'laranja']

```

## Compreens√µes de lista

As compreens√µes de lista s√£o uma maneira concisa de criar novas listas executando alguma opera√ß√£o em cada item em uma lista existente. A sintaxe b√°sica para uma compreens√£o de lista √© a seguinte:

```
nova_lista = [express√£o for item in lista if condi√ß√£o]

```

Por exemplo:

```
numeros = [1, 2, 3, 4, 5]
pares = [x for x in numeros if x % 2 == 0]
print(pares)  # Sa√≠da: [2, 4]

```

Abaixo est√£o alguns exemplos de list comprehension em Python:

Exemplo 1:

```
# Criar uma lista de n√∫meros pares de 0 a 10
pares = [x for x in range(11) if x % 2 == 0]
print(pares)

```

Output:

```
[0, 2, 4, 6, 8, 10]

```

Exemplo 2:

```
# Criar uma lista de n√∫meros √≠mpares de 0 a 10
impares = [x for x in range(11) if x % 2 == 1]
print(impares)

```

Output:

```
[1, 3, 5, 7, 9]

```

Exemplo 3:

```
# Criar uma lista de n√∫meros quadrados de 0 a 10
quadrados = [x**2 for x in range(11)]
print(quadrados)

```

Output:

```
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]

```

Exemplo 4:

```
# Criar uma lista de letras mai√∫sculas a partir de uma lista de strings
palavras = ['banana', 'ma√ß√£', 'laranja']
maiusculas = [letra.upper() for palavra in palavras for letra in palavra]
print(maiusculas)

```

Output:

```
['B', 'A', 'N', 'A', 'N', 'A', 'M', 'A', '√á', '√É', 'L', 'A', 'R', 'A', 'N', 'J', 'A']

```

Exemplo 5:

```
# Criar uma lista de elementos √∫nicos a partir de uma lista com elementos repetidos
repetidos = [1, 2, 3, 4, 3, 2, 1]
unicos = list(set(repetidos))
print(unicos)

```

Output:

```
[1, 2, 3, 4]

```

Podemos validar se um ou mais itens est√£o presentes em uma lista em Python de diversas formas, algumas delas incluem:

- Utilizar um loop for para iterar sobre a lista e verificar se cada item est√° presente utilizando a cl√°usula `in`

```
minha_lista = ['item1', 'item2', 'item3']
for item in minha_lista:
    if item == 'item1':
        print('O item1 est√° presente na lista!')

```

A fun√ß√£o `any()` retorna True se pelo menos um dos elementos na lista passada como argumento for True. Caso contr√°rio, retorna False. No exemplo mencionado, a fun√ß√£o `any()` √© usada em conjunto com uma lista de condi√ß√µes para verificar se pelo menos um item est√° presente em outra lista. Se pelo menos um item estiver presente, a condi√ß√£o retorna True e a mensagem "Pelo menos um dos itens est√° presente na lista!" √© impressa. Caso contr√°rio, a condi√ß√£o retorna False e a mensagem n√£o √© impressa.

```
minha_lista = ['item1', 'item2', 'item3']
if any(item in minha_lista for item in ['item1', 'item4']):
    print('Pelo menos um dos itens est√° presente na lista!')

```

A fun√ß√£o `set()` √© uma das fun√ß√µes integradas do Python que cria um conjunto, que √© uma cole√ß√£o de elementos √∫nicos. Em outras palavras, um conjunto √© uma cole√ß√£o n√£o ordenada de objetos exclusivos. Podemos criar um conjunto a partir de uma lista ou de qualquer outra sequ√™ncia usando a fun√ß√£o `set()`. Por exemplo:

```
frutas = ['ma√ß√£', 'banana', 'laranja', 'ma√ß√£']
conjunto_frutas = set(frutas)
print(conjunto_frutas)  # Sa√≠da: {'banana', 'laranja', 'ma√ß√£'}

```

Observe que o conjunto resultante cont√©m apenas uma ocorr√™ncia de cada elemento da lista original. Al√©m disso, os elementos do conjunto s√£o exibidos em uma ordem n√£o definida.

Podemos usar conjuntos para executar opera√ß√µes de conjunto, como uni√£o, interse√ß√£o e diferen√ßa. Por exemplo:

```
A = {1, 2, 3, 4}
B = {3, 4, 5, 6}
uniao = A.union(B)
print(uniao)  # Sa√≠da: {1, 2, 3, 4, 5, 6}
intersecao = A.intersection(B)
print(intersecao)  # Sa√≠da: {3, 4}
diferenca = A.difference(B)
print(diferenca)  # Sa√≠da: {1, 2}

```

Na primeira linha, criamos dois conjuntos A e B. Em seguida, usamos o m√©todo `union()` para criar um novo conjunto que cont√©m todos os elementos de A e B. Na terceira linha, usamos o m√©todo `intersection()` para criar um novo conjunto que cont√©m apenas os elementos que est√£o em ambos A e B. Na quinta linha, usamos o m√©todo `difference()` para criar um novo conjunto que cont√©m apenas os elementos que est√£o em A, mas n√£o em B.

Os conjuntos s√£o uma estrutura de dados poderosa e flex√≠vel em Python, e podem ser usados em uma variedade de contextos, incluindo filtragem de dados, remo√ß√£o de duplicatas e c√°lculo de estat√≠sticas.

- Utilizar a fun√ß√£o `set()` para transformar a lista em um conjunto, e ent√£o verificar se a interse√ß√£o entre o conjunto de itens que queremos encontrar e o conjunto da lista original √© maior que zero

```
minha_lista = ['item1', 'item2', 'item3']
itens_procurados = set(['item1', 'item4'])
if len(set(minha_lista) & itens_procurados) > 0:
    print('Pelo menos um dos itens est√° presente na lista!')

```

Essas s√£o apenas algumas das formas de validar se um ou mais itens est√£o presentes em uma lista em Python.

### **Dicion√°rio**

## Dicion√°rios em Python

Dicion√°rios s√£o uma estrutura de dados em Python que permite armazenar valores associados a chaves. Em outras palavras, √© como se fosse uma lista de pares chave-valor. A sintaxe para criar um dicion√°rio √© a seguinte:

```
meu_dict = {"chave1": "valor1", "chave2": "valor2", "chave3": "valor3"}

```

As chaves s√£o strings ou n√∫meros, e os valores podem ser qualquer tipo de dado em Python, incluindo outros dicion√°rios. Por exemplo:

```
meu_dict = {"nome": "Jo√£o", "idade": 25, "telefones": {"celular": "1234-5678", "residencial": "4321-8765"}}

```

Podemos acessar os valores do dicion√°rio atrav√©s das suas chaves. Por exemplo:

```
print(meu_dict["nome"])
# Sa√≠da: Jo√£o

print(meu_dict["telefones"]["celular"])
# Sa√≠da: 1234-5678

```

Tamb√©m podemos adicionar, modificar e remover itens do dicion√°rio. Para adicionar um novo item, basta criar uma nova chave e atribuir um valor a ela:

```
meu_dict["email"] = "joao@email.com"

```

Para modificar um item existente, basta atribuir um novo valor √† chave correspondente:

```
meu_dict["idade"] = 26

```

Para remover um item, podemos usar o m√©todo `pop()`:

```
meu_dict.pop("telefones")

```

Por fim, podemos percorrer um dicion√°rio usando o loop `for`. Por exemplo:

```
for chave, valor in meu_dict.items():
    print(chave + ": " + str(valor))

```

A sa√≠da seria:

```
nome: Jo√£o
idade: 26
email: joao@email.com
```

## **Pandas**

### **Iniciando**

Pandas √© a biblioteca focada em BANCO DE DADOS. Sua sintaxe b√°sica √© a seguinte:

```python
import pandas as pd
```

Lembrando que o pd ali escrito, pode ser qualquer outra coisa, mas sempre que uma sintaxe dando call em algo do pandas for escrito, precisar√° fazer essa altera√ß√£o 

Exemplo:

```python
import pandas as pandas
```

```python
import pandas as pan
```

O mais comum √© usar pd como no primeiro exemplo 

### **Criando dados do zero:**

Para fazer isso, podemos usar a seguinte linha:

```python
pd.DataFrame({'Yes':[50,21],'No':[131,2]})
```

Output:

|  | Yes | No |
| --- | --- | --- |
| 0 | 50 | 131 |
| 1 | 21 | 2 |

```python
Abrevia√ß√£odadaaopacote.DataFrame({'nome da coluna 1':[conte√∫do coluna 1],'nome coluna 2':[conte√∫do coluna 2]})
```

Por√©m, podemos ver uma certa peculiaridade que quero apontar:

A contagem come√ßa a partir da linha zero, uma diferen√ßa consider√°vel do Excel que come√ßa na linha 1, o que em certos momentos de selecionar x n√∫meros de linhas pode induzir a erro, ent√£o √© importante aten√ß√£o nesse detalhe

As colunas no c√≥digo criado tem nome, as linhas n√£o mas isso pode ser mudado:

```python
pd.DataFrame({'Yes':[50,21],'No':[131,2]},index='oferta A','oferta B')
```

Output:

|  | Yes | No |
| --- | --- | --- |
| Oferta A | 50 | 131 |
| Oferta B | 21 | 2 |

Basicamente, a sintaxe √© a seguinte:

```python
abrevia√ß√£odonomedopacote.DataFrame({'nome coluna 1':[conteudos coluna 1],'nome coluna 2':[conte√∫dos coluna 2]},index='nome linha 1')
```

### **Ler arquivos**

O mais interessante dessa pacote √© a forma de tratar os dados, o Pandas consegue buscar esses arquivos. √â bom lembrar que o Python vai buscar os dados, ou seja, n√£o quer dizer que n√£o v√° puxar tabelas din√¢micas ou gr√°ficos, mas ele vira em um formato ileg√≠vel, por isso √© importante tentar buscar planilhas apenas com os dados

Bom, como podemos ler os arquivos no Python com o Pandas? E para visualizar esse arquivo?

Bom, a√≠ temos a ideia mais b√°sica do Python:

Salvar dados em vari√°veis e visualizar, nos dois exemplos acima vemos que o arquivo Excel est√° dentro de uma vari√°vel ‚Äúdf‚Äù

```python
import pandas as pd
variavel = abrevia√ß√£odonomedopacote.read_formato (r'caminho para o arquivo\nome do arquivo.formato excel')
print(df)
```

Um exemplo:

```python
import pandas as pd
df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx')
print(df)
```

Ao tentar rodar, √© poss√≠vel que aconte√ßa um erro:

ImportError: Missing optional dependency ‚Äòxlrd‚Äô

Nesse caso, √© preciso instalar um pacote a mais:

```python
pip install openpyxl
```

Nao se preocupe, voltaremos nesse pacote mais tarde, ele ser√° relevante para outras funcionalidades.

### **Selecionar planilhas dentro de um arquivo, colunas,filtros e substitui√ß√£o de dados**

Aqui √© onde as coisas come√ßam a ficar interessantes, o Pandas √© uma √≥tima ferramenta para automa√ß√£o, at√© mais que o VBA, podendo ser mais r√°pido e mais preciso para TRABALHAR DADOS

Sabemos que n√£o vivemos em um mundo ideal e perfeito em que simplesmente v√£o enviar\produzir um Excel que n√£o tenha algum tipo de planilha dentro dele al√©m da que precisamos, a planilha com banco de dados necess√°rios, uma com tabelas din√¢micas, outra com gr√°ficos e por a√≠ vai, ou seja, com mais de uma aba!!!

Aqui vou apresentar duas formas de fazer essa ‚Äúsele√ß√£o‚Äù da(s) abas que estamos buscando, pelo nome da planilha e pelo n√∫mero que ela representa dentro dela:

```python
df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name='Aba 2'))

#ou

df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name=2))

#ou

df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name=None))
```

O que est√° ocorrendo nos exemplos acima?

Basicamente o mesmo que no cap√≠tulo anterior ‚ÄúLer arquivos‚Äù,mas, com uma diferen√ßa fundamental, est√° selecionando qual aba da planilha est√° ‚Äúpuxando‚Äù

A √∫nica exce√ß√£o √© a terceira linha do exemplo acima, nela, ele vai puxar todas as abas mas n√£o necessariamente elas estar√£o utiliz√°veis 

```python
variavel = abrevia√ß√£odonomedopacote.read_formato (r'caminho para o arquivo\nome do arquivo.formato excel',sheet_name=nome da aba ou n√∫mero √≠ndice correspondente))
print(df)
```

Lembrando que: se puxar por n√∫mero, a contagem come√ßa por 0, ent√£o o python vai identificar a primeira aba como zero, a segunda como 1, a terceira como 2 e por a√≠ vai.

Da mesma forma que conseguimos selecionar as abas, podemos selecionar as colunas que desejamos, √© interessante pois a partir daqui, conseguimos diminuir o uso de mem√≥ria do computador, s√£o os primeiros passos da an√°lise de dados.

A linha de racioc√≠nio n√£o mudou, o c√≥digo continua praticamente o mesmo, com a diferen√ßa que passaremos a selecionar as colunas.

```python
df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name='Aba 2', usecol=[3,4))

#ou

df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name='Aba 2', usecol=[3,4:7))

#ou

df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name='Aba 2', usecol="Coluna 3, Coluna 4"))

#ou

df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name='Aba 2', usecol="Coluna 3, Coluna 4:Coluna 7"))
```

Bom, o que mostram os exemplos acima?

As duas primeiras linhas est√£o selecionando, na ‚ÄúAba 2‚Äù, certas colunas espec√≠ficas baseadas em seus n√∫meros de colunas. No primeiro caso, as colunas 3 e 4 (lembrando que a contagem no python come√ßa no zero, na pr√°tica s√£o as colunas 4 e 5), enquanto o segundo caso seleciona as colunas 3 e as do intervalo de 4 a 7 ( novamente reafirmo, a contagem no python come√ßa no zero, ou seja, est√° selecionando na pr√°tica as colunas 4 e as que estiverem no intervalo das colunas 5 a 8).

A l√≥gica √© como essa abaixo:

| Coluna\linha Python | Coluna\linha Excel |
| --- | --- |
| 0 | 1 |
| 1 | 2 |
| 2 | 3 |
| 3 | 4 |

E a sintaxe √© a seguinte:

```python
variavel = abrevia√ß√£odonomedopacote.read_formato (r'caminho para o arquivo\nome do arquivo.formato excel',sheet_name=nome da aba ou n√∫mero √≠ndice correspondente, usecol= nome ou n√∫meros das colunas)
```

Algo legal, nesse mesmo formato que podemos usar √© a fun√ß√£o nrows, nela selecionamos um x n√∫meros de linhas para pularmos. Pode ser interessante usar em database que se come√ßa fora da primeira linha.

Nesse exemplo, estamos dizendo o seguinte ao python:

Vari√°vel df, acesse o caminho ‚ÄúC:user\Lucas\downloads\file\‚Äù e abra o receba as colunas 3 e 4 da Aba 2, mas, pule as 11 primeiras linhas. Ou seja, apenas a partir da 12¬™ linha.

Lembrando que a contagem no python come√ßa na linha zero

Para finalizar esse cap√≠tulo, vamos tratar filtros:

```python
df = pd.read_excel (r'C:user\Lucas\downloads\file\File name.xlsx',sheet_name='Aba 2', usecol=[3,4),nrows=11)
```

Por fim, filtragem!

Fazer filtros com os databases √© uma das formas mais b√°sicas de trabalhar dados, obviamente que conseguimos fazer isso python, antes e chegarmos nisso, √© interessante saber, como selecionar linhas e colunas de uma base de dados.

Para selecionar colunas, temos 2 modos:

```python
df.dados
```

```python
df['dados']
```

No que digitei acima, parto da ideia da exist√™ncia de um banco de dados/data frame chamado **df** e que ele tem uma coluna chama **dados**, dessa forma estamos selecionando ela. 

E para selecionar a primeira linha de **df**? E para selecionar a primeira linha de **df** da coluna dados?

```python
df.dados[0]
```

```python
df['dados'][0]
```

Basicamente a sintaxe √© o seguinte:

```python
dataframe.coluna[linha]

#ou

dataframe['coluna'][linha]
```

O interessante √©, isso tudo que foi explicado, vem diretamente das filtragem de python como l√≠ngua nativa e n√£o do pandas, algumas vezes nem ser√° necess√°rio usar o Pandas para filtragens mas em outras ser√° extremamente necess√°rio

Ent√£o, vamos ao pandas. Ele usa duas ferramentas, loc e iloc e diferente dos esquemas anteriores, ele usa linhas primeiro e colunas depois

Vamos a alguns exemplos com iloc em que suas explica√ß√µes estarei comentadas em ######

```python
df.iloc[0]
##nesse primeiro caso, o iloc estar√° trazendo a primeira linha de frutas as colunas

df.iloc[:,0]
##nesse segundo, o iloc traz todas as kinhas da primeira coluna
##lembrando que quando usamos ":" podemos trazer uma s√©rie de intervalos
## ":"traz tudo do intervalo
## "0:10"traz tudo do intervalo entre as linhas 1 e 10
## "10:" traz tudo a partir da linha 10
## ":10" traz tudo anterior a linha 10

df.iloc[1:33,0]
#nesse exemplo, ele estar√° trazendo tudo da linha 1 a 32 da primeira coluna

df.iloc[:44,2:4]
#nesse outro, estamos trazendo tudo a partir da primeira linha at√© a linha 43, das colunas 1 a 
```

Com rela√ß√£o as colunas, obviamente n√£o precisamos trazer tudo a partir de n√∫meros, podemos usar os nomes delas

```python
df.iloc[1:33,'dados']
#Aqui estamos selecionando tudo da linha 1 a 32 da coluna dados

```

Mas ent√£o, qual a diferen√ßa entre loc e iloc?

Bom, loc √© costumeiramente mais usado para sele√ß√£o de valores quando os r√≥tulos tem import√¢ncia

Al√©m disso, ele seleciona exatamente o intervalo solicitado. Ou seja, vamos usar o intervalo de 0:1000

 

```python
df.iloc[0:1000]
```

Ele ir√° contar 1000 linhas, mas em python a contagem come√ßa a partir da linha 0, ou seja, ele mostrar√° at√© a linha 999

```python
df.loc[0:1000]
```

O python n√£o vai fazer a contagem de linhas mas sim, selecionar o intervalo pr√©-definido, ou seja, novamente, como a contagem come√ßa no 0, ele vai parar na linha 1000, tendo assim 1001 linhas

Ent√£o, vamos partir para o loc, agora tendo uma ou mais condi√ß√µes para filtrar os dados

Se fizermos o seguinte:

```python
df.dados == 'Brasil'
```

Vamos fingir que a coluna dados s√£o pa√≠ses, nesse caso acima, a filtragem funcionar√°? Bom, a resposta √© sim e n√£o!

Conseguiremos ver os valores que s√£o Brasil, mas em formato bool booleano (TRUE & FALSE), al√©m disso, teremos apenas essa coluna.

Ent√£o, como poder√≠amos fazer um filtro como do Excel? Bom, a√≠ j√° fica um pouco mais complicado

Vamos as sintaxes:

```python
df.loc[df.dados == 'Brasil']
```

Bom, para fazer a filtragem b√°sica, usamos desse modo acima, a sintaxe b√°sica √© essa

```python
nomedodataframe.loc[nomedodataframe.nomedacoluna == input desejado]
```

Para filtrar mais de uma coluna

```python
df.loc[(df.dados == 'Brasil') & (df.numeros >= 15)]
```

```python
nomedodataframe.loc[(nomedodataframe.nomedacoluna == input desejado) & (nomedodataframe.nomedacoluna2 == input desejado)]
```

Para filtrar uma coluna ou outra

```python
df.loc[(df.dados == 'Brasil') | (df.numeros >= 15)]
```

```python
nomedodataframe.loc[(nomedodataframe.nomedacoluna == input desejado) | (nomedodataframe.nomedacoluna2 == input desejado)]
```

Para filtrar mais de um valor na mesma coluna

```python
df.loc[df.dados.isin(['Brasil','Portugal'])
```

```python
nomedodataframe.loc[nomedodataframe.nomedacoluna.isin([input desejado])
```

Para filtrar todos os valores, menos x ou y

```python
df.loc[~df.dados.isin(['Brasil','Portugal'])
```

```python
nomedodataframe.loc[~nomedodataframe.nomedacoluna.isin([input desejado])
```

Por fim, para substituir dados:

De toda uma coluna

```python
df['dados']='Umdoistres'
```

Com esse c√≥digo, todos os dados da coluna dados ser√£o substitu√≠dos por umdoistres

```python
df['dados']['Brasil']='Umdoistres'
```

E com o c√≥digo acima, apenas as linhas onde estiver escrito Brasil na coluna dados, ser√£o substitu√≠dos por umdoistres

```python
nome do dataframe[nome da coluna][dado a ser substituido]= novo dado
```

### **Fun√ß√µes summary e maps**

No cap√≠tulo anterior vimos como manipular dados, selecionando colunas e aplicando filtros, obviamente foi apenas o come√ßo. Aqui, vamos ver um pouco mais sobre tratamento de dados com as fun√ß√µes summary e maps

Por limita√ß√µes do Notion, n√£o temos como rodar c√≥digos aqui, ent√£o ser√° algo mais te√≥rico

```python
df.coluna1.describe()
```

Bom, a fun√ß√£o describe vai extrair da coluna de um dataframe algumas informa√ß√µes, como contagem, m√©dia, m√°ximo, m√≠nimo, frequ√™ncia,tipo de dados, etc. Mas isso pode depender se os dados forem strings ou num√©ricos

Alguns outros exemplos que podemos usar s√£o mean, unique e value_counts(). Ficaria algo mais ou menos assim:

```python
df.coluna1.mean()

df.coluna1.unique()

df.coluna1.value_counts()
```

Ent√£o, vamos falar um pouco sobre a fun√ß√£o maps, a principal id√©ia relacionado a essa fun√ß√£o √© que ela mapeie e aplique um certo crit√©rio linha a linha. Isso  poss√≠vel se utilizando de um loop e dando call em uma fun√ß√£o ou utilizando lambda

Vamos ver as duas formas utilizando o seguinte exemplo

```python
precos = [1000,1400,1125,2250]

def adicionar_imposto(preco):
   return preco √ó 1.1

precos_com_imposto=[]
for precos in precos:
   precos_com_imposto.append(adicionar_imposto(preco))
print(precos_com_imposto)
```

O c√≥digo acima est√° aplicando um m√©todo de adicionar impostos em certos valores de uma vari√°vel pr√© definida, para isso, criou-se uma fun√ß√£o para adicionar esses impostos.

Em seguida, para vermos os impostos aplicados em todos os pre√ßos, foi usado um la√ßo for dando call nessa fun√ß√£o anterior, junto ao append e em seguida, o print

Agora, como essa fun√ß√£o ficaria com o map?

```python
precos = [1000,1400,1125,2250]
def adicionar_imposto(preco):   
   return preco √ó 1.1

precos_com_imposto=list(map(adicionar_imposto,precos))
print(precos_com_imposto)
```

A fun√ß√£o lambda vem com a miss√£o de simplificar ainda mais o que a map j√° faz, a diferen√ßa, n√£o vamos precisar definir uma fun√ß√£o anterior

Lambda tem a seguinte sintaxe

```python
lambda x:x*1,1
```

Ela recebe o valor desejado no x anterior aos : e o valor dado como resposta ser√° o segundo x

Aplicando a fun√ß√£o map, o resultado √© o seguinte

```python
precos = [1000,1400,1125,2250]
precos_com_imposto=list(map(lambda x:x*1.1, precos))
print(precos_com_imposto)
```

√â uma forma mais r√°pida e mais pr√°tica de fazer o resultado acontecer

### **Agrupamento e ordena√ß√£o**

Agrupar √© uma forma de conseguir organizar os dados de acordo com a visualiza√ß√£o e ordem que voc√™ deseja trazer. 

A ideia √© usar o ‚Äúgroupby()‚Äù atrelado com algo mais, com valores, m√©dias, contagem, enfim.

Vou tentar passar a usar esse arquivo abaixo para os exemplos

[Database.xlsx](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Database.xlsx)

Ap√≥s fazer o download desse arquivo, ajuste o caminho dele na linha 2 de acordo com o local que o salvou.

```python
import pandas as pd
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
dados=dados.groupby('Item').Item.count()

print(dados)
```

Esse √© um exemplo bem simples, o resultado ser√° um agrupamento dos itens pelas quantidades

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled.png)

Da mesma forma que fizemos esse agrupamento pela contagem dos itens, podemos fazer tamb√©m usando outras ferramentas, nos dois pr√≥ximos c√≥digos usaremos o valor minimo e m√°ximo3 atrelado a regi√£o

```python
import pandas as pd
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
dados=dados.groupby('Region').Cost.min()

print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%201.png)

```python
import pandas as pd
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
dados=dados.groupby('Region').Cost.max()

print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%202.png)

Mais um m√©todo interessante usando o groupby √© com o a fun√ß√£o agg, que permite a n√≥s rodarmos v√°rias fun√ß√µes diferentes de forma simult√¢nea 

```python
from statistics import mean
import pandas as pd
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
dados=dados.groupby('Region').Cost.agg([len,max,min,mean])

print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%203.png)

Al√©m disso, podemos usar uma, duas, tr√™s colunas, quantas quisermos no groupby, segue o exemplo

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
print(type(dados.Cost))

dados1=dados.groupby(['Region','Item']).Total.agg([len])
display(dados1)

print(dados1)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%204.png)

E para finalizar, podemos definir as ordens que queremos dos n√∫meros, o c√≥digo abaixo colocamos para organiza-los a partir de Regi√£o e Len, de forma que seja do maior para o menor (para isso serve o c√≥digo ascending mas que por padr√£o, vem como True no c√≥digo)

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

dados1=dados.groupby(['Region','Item']).Total.agg([len])
dados1=dados1.sort_values(by=['Region','len'], ascending=False)
display(dados1)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%205.png)

### **Tipos de dado, Valores NA e Renomear**

Para conseguir pegar os tipos de dados de uma coluna, usamos uma fun√ß√£o bem simples chamada ‚Äúdtype‚Äù. Al√©m disso, √© poss√≠vel converter todos os dados de uma coluna

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

print(dados.Total.dtype)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%206.png)

Al√©m disso, podemos ver todos os tipos de dados de um DataFrame usando a mesma fun√ß√£o

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

print(dados.dtypes)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%207.png)

Podemos tamb√©m mudar todos os tipos de dados de uma fun√ß√£o, √© bem simples

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

dados.Units=dados.Units.astype('float64')

print(dados.dtypes)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%208.png)

Os missing values, ou os conhecidos NA‚Äôs possuem formas de serem tratados, podemos seleciona-los e substitui-los.

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

print(dados[pd.isnull(dados.Rep)])
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%209.png)

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

dados.Rep=dados.Rep.fillna("Desconhecido")
print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2010.png)

Para finalizar esse assunto, da mesma forma que podemos preencher e mudar linhas com NA‚Äôs, podemos tamb√©m substituir outros valores, no exemplo abaixo, estaremos substituindo ‚ÄúEast‚Äù por Leste na coluna regi√£o, mantendo o c√≥digo que escreve desconhecido na coluna ‚ÄúREP‚Äù

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

dados.Rep=dados.Rep.fillna("Desconhecido")
dados.Region=dados.Region.replace("East","Leste")
print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2011.png)

Outro ponto, podemos renomear as colunas e at√© mesmo linhas. No exemplo abaixo estamos mudando os nomes das colunas ‚ÄúorderDate‚Äù e ‚ÄúRep‚Äù. Como est√° sob colchetes, precisamos apenas separar por virgula, sem precisar de outros simbolos como ( ) ou [ ].  Usamos a fun√ß√£o rename e dentro dela podemos definir se a usaremos em colunas ou linhas, no caso, para colunas usamos o argumento: ‚Äúcolumns={‚Äônome atual‚Äô:‚Äônovo nome‚Äô} enquanto para linhas √© bem parecido, usamos o argumento ‚Äúindex={‚Äôn√∫mero da linha‚Äô:‚Äônome da linha‚Äô}. Seguem os exemplos

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

dados=dados.rename(columns={'Rep':'Nomes','OrderDate':'Data'})
print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2012.png)

```python
import pandas as pd
from IPython.display import display
dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)

dados=dados.rename(index={0:'Primeira',1:'Segunda'})
print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2013.png)

### **Combina√ß√µes**

Combina√ß√µes s√£o basicamente formas de fazer de-para, ou uma forma na programa√ß√£o de fazer procv/procx.

Temos dois m√©todos que s√£o bem conhecidos, Join e Merge, al√©m do concat.

- **Concat**
    
    Vamos come√ßar pela Concat, ela vem de concaternar ou seja, juntar as bases. Caso tenha uma base com os mesmos nomes de colunas e que deseje juntar, √© poss√≠vel fazer isso usando concat
    
    No exemplo abaixo, usaremos a mesma base, juntando ela com ela mesma. Como assim? Basicamente, estamos apenas renomeando uma nova vari√°vel puxando a mesma base, apenas para servir de exemplo
    
    ```python
    import pandas as pd
    from IPython.display import display
    dados=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
    dados1=pd.read_excel('C:\\Users\\lucas\\Downloads\\Database.xlsx',sheet_name=1)
    
    dados=pd.concat([dados,dados1])
    print(dados)
    ```
    
    ![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2014.png)
    
    Apesar de no exemplo acima termos apenas at√© a linha 42, o Python deixa claro que o DataFrame tem 86 linhas. E para concaternar lado a lado, basta usar pd.concat([dataframe1,dataframe2],axis=1)
    
- **Merge**
    
    O `merge` em Python √© uma fun√ß√£o utilizada para combinar/juntar dois DataFrames em uma √∫nica estrutura, semelhante a um JOIN em SQL. A fun√ß√£o `merge()` √© parte da biblioteca Pandas, amplamente usada para manipula√ß√£o e an√°lise de dados. Ela permite unir DataFrames de acordo com colunas ou √≠ndices em comum.
    
    ### 1. **Funcionamento do `merge`**
    
    A fun√ß√£o `merge()` combina linhas de dois DataFrames com base em colunas ou √≠ndices comuns. Ele suporta diferentes tipos de join:
    
    - **Inner Join**: Combina apenas as linhas onde h√° correspond√™ncia entre os DataFrames.
    - **Left Join**: Combina todas as linhas do DataFrame da esquerda, e apenas as correspondentes do DataFrame da direita.
    - **Right Join**: Combina todas as linhas do DataFrame da direita, e apenas as correspondentes do DataFrame da esquerda.
    - **Outer Join**: Combina todas as linhas de ambos os DataFrames, preenchendo os valores ausentes com NaN.
    
    ### 2. **Utiliza√ß√µes**
    
    - **Unir Dados de Fontes Diferentes**: Combinar tabelas relacionadas por meio de colunas comuns.
    - **Enriquecer um DataFrame**: Adicionar colunas com informa√ß√µes adicionais de outro DataFrame.
    - **Realizar An√°lises Cruzadas**: Fazer compara√ß√µes e an√°lises entre diferentes conjuntos de dados.
    
    ### 3. **Exemplos Pr√°ticos**
    
    ### **Exemplo B√°sico: Inner Join**
    
    Vamos supor que temos dois DataFrames contendo informa√ß√µes sobre clientes e suas ordens de compra.
    
    ```python
    import pandas as pd
    
    # DataFrame de clientes
    df_clientes = pd.DataFrame({
        'cliente_id': [1, 2, 3, 4],
        'nome': ['Ana', 'Bruno', 'Carlos', 'Daniela']
    })
    
    # DataFrame de ordens
    df_ordens = pd.DataFrame({
        'ordem_id': [101, 102, 103, 104],
        'cliente_id': [1, 2, 3, 3],
        'valor': [250, 120, 310, 150]
    })
    
    # Inner Join com base na coluna 'cliente_id'
    df_merged = pd.merge(df_clientes, df_ordens, on='cliente_id')
    print(df_merged)
    
    ```
    
    **Resultado:**
    
    ```
       cliente_id    nome  ordem_id  valor
    0           1     Ana       101    250
    1           2   Bruno       102    120
    2           3  Carlos       103    310
    3           3  Carlos       104    150
    
    ```
    
    ### **Exemplo Intermedi√°rio: Left Join**
    
    Agora, queremos garantir que todos os clientes apare√ßam no resultado, mesmo que n√£o tenham feito compras.
    
    ```python
    # Left Join para incluir todos os clientes
    df_merged_left = pd.merge(df_clientes, df_ordens, on='cliente_id', how='left')
    print(df_merged_left)
    
    ```
    
    **Resultado:**
    
    ```
       cliente_id    nome  ordem_id  valor
    0           1     Ana     101.0  250.0
    1           2   Bruno     102.0  120.0
    2           3  Carlos     103.0  310.0
    3           3  Carlos     104.0  150.0
    4           4  Daniela       NaN    NaN
    
    ```
    
    ### **Exemplo Avan√ßado: Merge em M√∫ltiplas Colunas e com Suporte a √çndices**
    
    Voc√™ pode querer unir DataFrames em mais de uma coluna ou at√© mesmo usando √≠ndices.
    
    ```python
    # DataFrame com √≠ndice
    df_produtos = pd.DataFrame({
        'produto_id': [1, 2, 3, 4],
        'categoria': ['Eletr√¥nicos', 'Roupas', 'Roupas', 'M√≥veis']
    }).set_index('produto_id')
    
    # DataFrame de vendas
    df_vendas = pd.DataFrame({
        'produto_id': [1, 2, 2, 3, 3],
        'cliente_id': [1, 2, 2, 3, 4],
        'quantidade': [1, 2, 3, 1, 2]
    })
    
    # Merge usando a coluna e o √≠ndice
    df_merged_advanced = pd.merge(df_vendas, df_produtos, left_on='produto_id', right_index=True)
    print(df_merged_advanced)
    
    ```
    
    **Resultado:**
    
    ```
       produto_id  cliente_id  quantidade    categoria
    0           1           1           1  Eletr√¥nicos
    1           2           2           2      Roupas
    2           2           2           3      Roupas
    3           3           3           1      Roupas
    4           3           4           2      Roupas
    
    ```
    
    ### **Exemplo com `suffixes`**
    
    Quando voc√™ est√° unindo DataFrames que possuem colunas com o mesmo nome, o `merge` adiciona sufixos para diferenciar as colunas duplicadas.
    
    ```python
    import pandas as pd
    
    # DataFrame de funcion√°rios
    df_empregados = pd.DataFrame({
        'id': [1, 2, 3],
        'nome': ['Alice', 'Bob', 'Charlie'],
        'departamento': ['TI', 'RH', 'Financeiro']
    })
    
    # DataFrame de gestores
    df_gestores = pd.DataFrame({
        'id': [1, 2, 4],
        'nome': ['Daniel', 'Emma', 'Frank'],
        'departamento': ['TI', 'RH', 'Vendas']
    })
    
    # Realizando o merge com suffixes
    df_merged_suffixes = pd.merge(df_empregados, df_gestores, on='departamento', suffixes=('_emp', '_gestor'))
    print(df_merged_suffixes)
    
    ```
    
    **Resultado:**
    
    ```
       id_emp    nome_emp departamento  id_gestor nome_gestor
    0       1       Alice          TI          1      Daniel
    1       2         Bob          RH          2        Emma
    
    ```
    
    Nesse exemplo, as colunas `id` e `nome`, que aparecem em ambos os DataFrames, s√£o diferenciadas pelos sufixos `_emp` e `_gestor`.
    
    ### **Tratamento de Duplicatas com `drop_duplicates`**
    
    Ap√≥s um merge, voc√™ pode acabar com duplicatas que precisam ser removidas. O m√©todo `drop_duplicates()` √© √∫til para isso.
    
    ```python
    # DataFrame de vendas
    df_vendas = pd.DataFrame({
        'cliente_id': [1, 2, 2, 3],
        'produto_id': [101, 102, 102, 103],
        'quantidade': [1, 2, 2, 1]
    })
    
    # DataFrame de clientes
    df_clientes = pd.DataFrame({
        'cliente_id': [1, 2, 3],
        'nome': ['Ana', 'Bruno', 'Carlos']
    })
    
    # Realizando o merge
    df_merged_duplicates = pd.merge(df_vendas.drop_duplicates(subset="produto_id"), df_clientes, on='cliente_id')
    
    # Removendo duplicatas
    print(df_merged_duplicates )
    
    ```
    
    **Resultado:**
    
    ```
       cliente_id  produto_id  quantidade    nome
    0           1        101           1    Ana
    1           2        102           2  Bruno
    3           3        103           1 Carlos
    
    ```
    
    Aqui, as duplicatas foram removidas ap√≥s o merge.
    
    ### **Sele√ß√£o de Colunas Durante o `merge`**
    
    √Äs vezes, voc√™ n√£o precisa de todas as colunas do DataFrame ap√≥s o merge. Voc√™ pode selecionar apenas as colunas desejadas diretamente na opera√ß√£o.
    
    ```python
    # DataFrame de produtos
    df_produtos = pd.DataFrame({
        'produto_id': [101, 102, 103],
        'nome_produto': ['Notebook', 'Camiseta', 'Cadeira'],
        'preco': [3000, 50, 200]
    })
    
    # DataFrame de vendas
    df_vendas = pd.DataFrame({
        'venda_id': [1, 2, 3],
        'produto_id': [101, 102, 103],
        'quantidade': [2, 5, 1]
    })
    
    # Realizando o merge e selecionando colunas espec√≠ficas
    df_merged_selected_columns = pd.merge(df_vendas, df_produtos[['produto_id', 'nome_produto']], on='produto_id')
    print(df_merged_selected_columns)
    
    ```
    
    **Resultado:**
    
    ```
       venda_id  produto_id  quantidade nome_produto
    0         1        101           2     Notebook
    1         2        102           5     Camiseta
    2         3        103           1      Cadeira
    
    ```
    
    Neste exemplo, ap√≥s o merge, apenas as colunas `produto_id` e `nome_produto` do DataFrame `df_produtos` foram selecionadas.
    
    ### **Exemplo Combinado: `suffixes`, `drop_duplicates`, e Sele√ß√£o de Colunas**
    
    Voc√™ pode combinar todas essas t√©cnicas para manipular seus DataFrames de forma eficiente.
    
    ```python
    # DataFrame de produtos
    df_produtos = pd.DataFrame({
        'produto_id': [101, 102, 103, 104],
        'nome_produto': ['Notebook', 'Camiseta', 'Cadeira', 'Mesa'],
        'categoria': ['Eletr√¥nicos', 'Roupas', 'M√≥veis', 'M√≥veis'],
        'preco': [3000, 50, 200, 350]
    })
    
    # DataFrame de vendas
    df_vendas = pd.DataFrame({
        'venda_id': [1, 2, 3, 4, 5],
        'produto_id': [101, 102, 102, 103, 104],
        'quantidade': [2, 5, 5, 1, 2]
    })
    
    # Realizando o merge com suffixes e selecionando colunas
    df_merged_combined = pd.merge(
        df_vendas,
        df_produtos[['produto_id', 'nome_produto', 'categoria']].drop_duplicates(subset='produto_id'),
        on='produto_id',
        suffixes=('_venda', '_produto')
    )
    
    print(df_merged_combined)
    
    ```
    
    **Resultado:**
    
    ```
       venda_id  produto_id  quantidade nome_produto   categoria
    0         1        101           2     Notebook  Eletr√¥nicos
    1         2        102           5     Camiseta      Roupas
    3         4        103           1      Cadeira      M√≥veis
    4         5        104           2         Mesa      M√≥veis
    
    ```
    
    Neste exemplo, usamos `suffixes` para diferenciar colunas, `drop_duplicates` para remover duplicatas, e selecionamos apenas as colunas necess√°rias para o resultado final.
    
    ### 4. **Combina√ß√µes de Merge**
    
    Al√©m dos exemplos b√°sicos, voc√™ pode explorar combina√ß√µes complexas utilizando o `merge()` com par√¢metros como `suffixes` para lidar com colunas de mesmo nome, ou combinando diferentes tipos de join para diferentes DataFrames.
    
    Vamos explorar algumas funcionalidades avan√ßadas do `merge()` em Python, incluindo o uso de `suffixes`, tratamento de duplicatas, e a sele√ß√£o de colunas diretamente na opera√ß√£o de merge.
    
- **Join**
    
    O `join` em Python, especificamente no contexto do Pandas, √© uma opera√ß√£o que permite combinar dois DataFrames com base em um √≠ndice comum. Enquanto o `merge` √© mais flex√≠vel e permite unir DataFrames com base em colunas, o `join` √© mais adequado para combina√ß√µes baseadas em √≠ndices.
    
    ### 1. **Funcionamento do `join`**
    
    A fun√ß√£o `join()` no Pandas √© utilizada para combinar dois DataFrames com base em seus √≠ndices. Voc√™ pode especificar o tipo de jun√ß√£o (inner, left, right, outer) e como lidar com colunas duplicadas.
    
    ### 2. **Utiliza√ß√µes**
    
    - **Unir DataFrames Baseados em √çndices**: Ideal para combinar DataFrames que compartilham um √≠ndice comum.
    - **Adicionar Informa√ß√µes a um DataFrame**: Similar ao `merge`, mas mais conveniente quando as tabelas j√° est√£o indexadas.
    
    ### 3. **Exemplos Pr√°ticos**
    
    ### **Exemplo B√°sico: Left Join**
    
    Neste exemplo, vamos combinar dois DataFrames usando um `left join` com base nos √≠ndices.
    
    ```python
    import pandas as pd
    
    # DataFrame de empregados
    df_empregados = pd.DataFrame({
        'nome': ['Ana', 'Bruno', 'Carlos', 'Daniela'],
        'departamento': ['TI', 'RH', 'Financeiro', 'TI']
    }, index=[1, 2, 3, 4])
    
    # DataFrame de sal√°rios
    df_salarios = pd.DataFrame({
        'salario': [7000, 6000, 5000],
        'bonus': [500, 400, 300]
    }, index=[1, 2, 3])
    
    # Left Join
    df_join_left = df_empregados.join(df_salarios, how='left')
    print(df_join_left)
    
    ```
    
    **Resultado:**
    
    ```
          nome departamento  salario  bonus
    1      Ana           TI   7000.0  500.0
    2    Bruno           RH   6000.0  400.0
    3   Carlos   Financeiro   5000.0  300.0
    4  Daniela           TI      NaN    NaN
    
    ```
    
    ### **Exemplo Intermedi√°rio: Inner Join**
    
    Um `inner join` combina apenas as linhas em que os √≠ndices est√£o presentes em ambos os DataFrames.
    
    ```python
    # Inner Join
    df_join_inner = df_empregados.join(df_salarios, how='inner')
    print(df_join_inner)
    
    ```
    
    **Resultado:**
    
    ```
         nome departamento  salario  bonus
    1     Ana           TI   7000.0  500.0
    2   Bruno           RH   6000.0  400.0
    3  Carlos   Financeiro   5000.0  300.0
    
    ```
    
    ### **Exemplo Avan√ßado: Join com `suffixes`**
    
    Se os DataFrames tiverem colunas com os mesmos nomes, voc√™ pode adicionar sufixos para diferenci√°-las.
    
    ```python
    # DataFrame de horas trabalhadas
    df_horas = pd.DataFrame({
        'horas_trabalhadas': [160, 150, 120, 140],
        'departamento': ['TI', 'RH', 'Financeiro', 'TI']
    }, index=[1, 2, 3, 4])
    
    # Join com sufixes
    df_join_suffixes = df_empregados.join(df_horas, lsuffix='_emp', rsuffix='_horas')
    print(df_join_suffixes)
    
    ```
    
    **Resultado:**
    
    ```
          nome_emp departamento_emp  horas_trabalhadas departamento_horas
    1        Ana               TI               160                  TI
    2      Bruno               RH               150                  RH
    3     Carlos      Financeiro               120           Financeiro
    4    Daniela              TI               140                  TI
    
    ```
    
    ### **Tratamento de Duplicatas e Sele√ß√£o de Colunas no Join**
    
    Voc√™ pode tratar duplicatas e selecionar colunas espec√≠ficas ap√≥s a opera√ß√£o de join.
    
    ```python
    # Join com sele√ß√£o de colunas espec√≠ficas e tratamento de duplicatas
    df_join_selected = df_empregados.join(df_horas[['horas_trabalhadas']], how='left').drop_duplicates(subset=['nome'])
    print(df_join_selected)
    
    ```
    
    **Resultado:**
    
    ```
          nome departamento  horas_trabalhadas
    1      Ana           TI               160
    2    Bruno           RH               150
    3   Carlos   Financeiro               120
    4  Daniela           TI               140
    
    ```
    
    ### 4. **Exemplo Combinado: Join Complexo**
    
    Agora, vamos combinar v√°rias t√©cnicas: sufixes, tratamento de duplicatas e sele√ß√£o de colunas em um √∫nico join.
    
    ```python
    # Join combinado com sufixes e sele√ß√£o de colunas
    df_join_combined = df_empregados.join(
        df_horas[['horas_trabalhadas', 'departamento']],
        how='outer',
        lsuffix='_emp',
        rsuffix='_horas'
    ).drop_duplicates(subset=['nome_emp'])
    
    # Sele√ß√£o de colunas espec√≠ficas
    df_join_final = df_join_combined[['nome_emp', 'departamento_emp', 'horas_trabalhadas']]
    print(df_join_final)
    
    ```
    
    **Resultado:**
    
    ```
          nome_emp departamento_emp  horas_trabalhadas
    1        Ana               TI               160
    2      Bruno               RH               150
    3     Carlos      Financeiro               120
    4    Daniela              TI               140
    
    ```
    
    - **Combina√ß√µes - Join e Map**
        
        Combinar a opera√ß√£o de `join` com `map` no Pandas pode ser muito √∫til para aplicar transforma√ß√µes ou adicionar informa√ß√µes em colunas de DataFrames antes ou depois do `join`. Vamos ver alguns exemplos pr√°ticos dessa combina√ß√£o.
        
        ### 1. **Exemplo B√°sico: Usando `map` Antes do `join`**
        
        Suponha que voc√™ tem um DataFrame com IDs e deseja substituir esses IDs pelos nomes correspondentes usando um dicion√°rio de mapeamento.
        
        ```python
        import pandas as pd
        
        # DataFrame de vendas com IDs de clientes
        df_vendas = pd.DataFrame({
            'cliente_id': [1, 2, 3, 4],
            'produto': ['Notebook', 'Camiseta', 'Cadeira', 'Mesa'],
            'quantidade': [1, 2, 3, 1]
        })
        
        # Dicion√°rio de mapeamento de IDs para nomes de clientes
        cliente_map = {
            1: 'Ana',
            2: 'Bruno',
            3: 'Carlos',
            4: 'Daniela'
        }
        
        # Aplicando map para substituir IDs por nomes
        df_vendas['cliente_nome'] = df_vendas['cliente_id'].map(cliente_map)
        
        print(df_vendas)
        
        ```
        
        **Resultado:**
        
        ```
           cliente_id   produto  quantidade cliente_nome
        0           1  Notebook          1         Ana
        1           2  Camiseta          2       Bruno
        2           3   Cadeira          3      Carlos
        3           4      Mesa          1     Daniela
        
        ```
        
        Neste exemplo, usamos `map` para substituir os IDs dos clientes pelos seus nomes antes de realizar um `join`.
        
        ### 2. **Exemplo Intermedi√°rio: Usando `join` e `map` Juntos**
        
        Agora, vamos fazer um `join` e, em seguida, aplicar `map` para criar uma nova coluna com base nas informa√ß√µes combinadas.
        
        ```python
        # DataFrame de departamentos
        df_departamentos = pd.DataFrame({
            'departamento_id': [1, 2, 3],
            'departamento': ['TI', 'RH', 'Financeiro']
        }, index=[1, 2, 3])
        
        # DataFrame de empregados com IDs de departamentos
        df_empregados = pd.DataFrame({
            'nome': ['Ana', 'Bruno', 'Carlos', 'Daniela'],
            'departamento_id': [1, 2, 3, 1]
        })
        
        # Join para combinar informa√ß√µes
        df_empregados_completo = df_empregados.join(df_departamentos, on='departamento_id')
        
        # Usando map para criar uma nova coluna de descri√ß√µes
        descricao_map = {
            'TI': 'Tecnologia da Informa√ß√£o',
            'RH': 'Recursos Humanos',
            'Financeiro': 'Departamento Financeiro'
        }
        
        df_empregados_completo['descricao'] = df_empregados_completo['departamento'].map(descricao_map)
        
        print(df_empregados_completo)
        
        ```
        
        **Resultado:**
        
        ```
              nome  departamento_id departamento                 descricao
        0      Ana               1           TI  Tecnologia da Informa√ß√£o
        1    Bruno               2           RH          Recursos Humanos
        2   Carlos               3   Financeiro  Departamento Financeiro
        3  Daniela               1           TI  Tecnologia da Informa√ß√£o
        
        ```
        
        Aqui, ap√≥s o `join`, aplicamos um `map` para adicionar uma coluna com a descri√ß√£o completa do departamento.
        
        ### 3. **Exemplo Avan√ßado: Mapeamento Condicional ap√≥s um Join**
        
        Suponha que voc√™ precise criar uma coluna que atribua um valor baseado em condi√ß√µes complexas ap√≥s um `join`.
        
        ```python
        # DataFrame de funcion√°rios
        df_funcionarios = pd.DataFrame({
            'funcionario_id': [1, 2, 3, 4],
            'nome': ['Ana', 'Bruno', 'Carlos', 'Daniela'],
            'departamento': ['TI', 'TI', 'RH', 'Financeiro']
        })
        
        # DataFrame de sal√°rios por departamento
        df_salarios = pd.DataFrame({
            'departamento': ['TI', 'RH', 'Financeiro'],
            'salario_base': [7000, 6000, 5000]
        })
        
        # Join baseado no departamento
        df_funcionarios_completo = df_funcionarios.join(df_salarios.set_index('departamento'), on='departamento')
        
        # Aplicando map para criar uma nova coluna de b√¥nus baseado no departamento
        bonus_map = {
            'TI': lambda x: x * 0.1,      # 10% de b√¥nus para TI
            'RH': lambda x: x * 0.15,     # 15% de b√¥nus para RH
            'Financeiro': lambda x: x * 0.2  # 20% de b√¥nus para Financeiro
        }
        
        df_funcionarios_completo['bonus'] = df_funcionarios_completo.apply(
            lambda row: bonus_map[row['departamento']](row['salario_base']), axis=1
        )
        
        print(df_funcionarios_completo)
        
        ```
        
        **Resultado:**
        
        ```
           funcionario_id     nome departamento  salario_base  bonus
        0               1      Ana           TI          7000   700.0
        1               2    Bruno           TI          7000   700.0
        2               3   Carlos           RH          6000   900.0
        3               4  Daniela   Financeiro          5000  1000.0
        
        ```
        
        Neste exemplo, usamos `apply` em conjunto com `map` para aplicar uma fun√ß√£o condicional baseada no departamento de cada funcion√°rio ap√≥s o `join`.
        
        ### 4. **Exemplo Combinado: Join, Map, e Tratamento de Duplicatas**
        
        Podemos combinar tudo: realizar um `join`, aplicar um `map`, e tratar duplicatas em um √∫nico fluxo de trabalho.
        
        ```python
        # DataFrame de projetos
        df_projetos = pd.DataFrame({
            'projeto_id': [1, 2, 3, 4],
            'departamento': ['TI', 'TI', 'RH', 'Financeiro'],
            'horas_trabalhadas': [120, 150, 100, 110]
        })
        
        # DataFrame de departamentos com IDs
        df_departamentos = pd.DataFrame({
            'departamento_id': [1, 2, 3],
            'departamento': ['TI', 'RH', 'Financeiro']
        }, index=[1, 2, 3])
        
        # Join para combinar informa√ß√µes
        df_projetos_completo = df_projetos.join(df_departamentos.set_index('departamento'), on='departamento', lsuffix='_proj', rsuffix='_dept')
        
        # Map para atribuir custo por hora baseado no departamento
        custo_hora_map = {
            'TI': 100,
            'RH': 80,
            'Financeiro': 90
        }
        
        df_projetos_completo['custo_total'] = df_projetos_completo['horas_trabalhadas'] * df_projetos_completo['departamento'].map(custo_hora_map)
        
        # Removendo duplicatas (se houvesse)
        df_projetos_final = df_projetos_completo.drop_duplicates()
        
        print(df_projetos_final)
        
        ```
        
        **Resultado:**
        
        ```
           projeto_id departamento_proj  horas_trabalhadas departamento_dept  departamento_id  custo_total
        0           1                TI                120                TI               NaN        12000
        1           2                TI                150                TI               NaN        15000
        2           3                RH                100                RH               NaN         8000
        3           4        Financeiro                110        Financeiro               NaN         9900
        
        ```
        
        Nesse exemplo, aplicamos o `map` para calcular o custo total com base nas horas trabalhadas e no custo por hora de cada departamento ap√≥s o `join`, e removemos duplicatas no final.
        
        Esses exemplos mostram como voc√™ pode combinar `join` e `map` para criar fluxos de trabalho poderosos e flex√≠veis no Pandas. Se precisar de mais exemplos ou quiser explorar outras possibilidades, estou √† disposi√ß√£o!
        
    

### **Adicional: Fun√ß√µes para colunas**

**** [python - How can I use the apply() function for a single column? - Stack Overflow](https://stackoverflow.com/questions/34962104/how-can-i-use-the-apply-function-for-a-single-column)

Bom, qual a ideia aqui: Basicamente, voc√™ tem colunas com valor x, y e z. Mas a coluna z1 dever√° ser a soma das colunas y e z, isso √© poss√≠vel ser feito? De quais formas podemos fazer isso?

Bom, a mais interessante √© usar a fun√ß√£o lambda

- Lambda com apply
    
    Bom, diferentemente do anterior, a mesma coisa pode ser feita mas em menos linhas, s√£o as chamadas fun√ß√µes an√¥nimas, sem necessariamente precisarmos nomear.
    
    ```jsx
    import pandas as pd
    
    df = pd.read_csv("C:\\Users\\lucas\\Downloads\\insurance.csv")
    
    df["nova coluna"]=df.apply(lambda x: x['age']*x['bmi'],axis=1)
    
    print(df["nova coluna"])
    ```
    
    Quando voc√™ adaptar o caminho do arquivo para seu computador e testa-lo, vai ter o seguinte retorno:
    
    ![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2015.png)
    
    Outra forma, ainda usando lambda, √© aplicando uma fun√ß√£o pr√©-definida, como no c√≥digo abaixo:
    
    ```jsx
    import pandas as pd
    
    df = pd.read_csv("C:\\Users\\lucas\\Downloads\\insurance.csv")
    
    def applyfunc1(x,y):
        return x*y
    
    df["col1"] = df.apply(lambda x: applyfunc1(x['age'],x['bmi']),axis=1)
    
    print(df["col1"])
    ```
    
    Aqui, definimos que 2 colunas (x e y) iriam se multiplicar por meio de uma fun√ß√£o separada, em seguida utilizamos o lambda para fazer sua aplica√ß√£o na nova coluna que definimos
    

## **Datavis**

### **Gr√°ficos de linhas**

- **Arquivos**
    
    [fifa.csv](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/fifa.csv)
    

Datavis vem de Data Visualization e possui algumas bibliotecas, nos exemplos da trilha, s√£o usadas as bibliotecas a seguir:

```python
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
print("Setup Complete")
```

Por√©m n√£o consegui fazer a seaborn rodar no Jupyter que eu havia criado, assim como a %matplotlib n√£o rodou nos offlines (Pycharm e Visual Studio) e n√£o encontrei solu√ß√µes pra isso, mas pelo menos at√© ent√£o, n√£o me atraapalhou em nada.

Vamos usar o mesmo exemplo da trilha

```python
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\fifa.csv', index_col='Date', parse_dates=True)
print(dados.head())

# Set the width and height of the figure
plt.figure(figsize=(16,6))

# Line chart showing how FIFA rankings evolved over time 
sns.lineplot(data=dados)
plt.show()
```

![Figure_1.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Figure_1.png)

Como podemos ver, os dados est√£o bagun√ßados‚Ä¶ Como podemos selecionar 2 ou mais pa√≠ses dessa lista para analisarmos? Bom, a resposta √© simples, veja no exemplo abaixo

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\fifa.csv', index_col='Date', parse_dates=True)
print(dados.head())

# Set the width and height of the figure
plt.figure(figsize=(16,6))

# Line chart showing how FIFA rankings evolved over time from Brazil and Argentina
sns.lineplot(data=dados['ARG'],label='Argentina')
sns.lineplot(data=dados['BRA'],label='Brasil')
plt.show()
```

O segredo est√° nas √∫ltimas linhas, selecionamos quais colunas queremos usar, da mesma forma que fizemos com o Pandas algumas vezes. Al√©m disso, temos um novo fator chamado ‚ÄúLabel‚Äù, ele serve para inserirmos a legenda e dizermos o que significa a linha que ele est√° plotando

### **Gr√°ficos** **de Colunas**

- Arquivos
    
    [flight_delays.csv](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/flight_delays.csv)
    

Bom, como sugerido, vamos aprender um pouco sobre gr√°fico de colunas!!!

A sintaxe √© simples, quase a mesma que do anterior, a diferen√ßa √© que usaremos sns.barplot

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\flight_delays.csv', index_col='Month')
print(dados.head())

# Set the width and height of the figure
plt.figure(figsize=(16,6))

# Line chart showing how FIFA rankings evolved over time 
plt.title('Average Arrival Delay for Spirit Airlines Flights, by Month')

sns.barplot(x=dados.index ,y=dados['NK'])
plt.ylabel("Arrival delay (in minutes)")
plt.xlabel('M√™s')

plt.show()
```

Vamos l√°, o que est√° acontecendo no c√≥digo acima? 

Bom, como esperado est√° criando um gr√°fico de colunas, al√©m do j√° padronizado linha de dados, somos apresentados a dois novos fatores, a nomear titulo, coluna Y e coluna x

A sintaxe, como esperada √© bem simples, voc√™ define qual eixo vai nomear e simplesmente digita as strings. O resultado √© o gr√°fico abaixo

![Airplanes.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Airplanes.png)

### **Mapa de Calor**

Seguindo a mesma linha dos gr√°ficos de colunas, al√©m disso, usando os mesmos dados, vamos criar um mapa de calor para conseguirmos analisar os dados de atraso de todas as empresas a√©reas que l√° est√£o listadas

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\flight_delays.csv', index_col='Month')
print(dados.head())

# Set the width and height of the figure
plt.figure(figsize=(16,6))

# Heatmap showing average arrival delay for each airline by month
sns.heatmap(data=dados, annot=True)

# Add label for horizontal axis
plt.xlabel("A√©rea")
plt.ylabel("M√™s")

plt.show()
```

Como podemos analisar at√© aqui, os c√≥digos para plotagem de gr√°ficos s√£o muito semelhantes, por√©m gostaria de chamar aten√ß√£o para um detalhe em especial do mapa de calor, a fun√ß√£o ‚ÄúAnnot‚Äù.

Ela torna fundamental a visualiza√ß√£o dos dados de forma mais clara, os n√∫meros passam a ficar visiveis com sua fun√ß√£o sendo True, como nos exemplos abaixo

Annot = False

![Annot False.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Annot_False.png)

Annot True

![Annot True.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Annot_True.png)

Fica claro quando aberta a imagem, o grande impacto dessa fun√ß√£o em particular

### **Gr√°ficos de Dispers√£o**

- Arquivo
    
    [insurance.csv](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/insurance.csv)
    

Os gr√°ficos de dispers√£o s√£o relacionados a estat√≠stica, eles mostram de forma visual rela√ß√µes entre uma ou mais vari√°veis. Entender esse tipo de gr√°fico for√ßa com que pelo menos um m√≠nimo no dom√≠nio de estat√≠stica tenha que ter sido aprendido

O banco de dados que estaremos usando para esses exemplos √© composto por ‚ÄúIdade, sexo, bmi (imc), filhos, fumante, regi√£o, taxas‚Äù

A primeira an√°lise que faremos ser√° usando as taxas pagas a companhia de seguro junto ao imc, para identificarmos se existe alguma correla√ß√£o

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\insurance.csv')
print(dados.head())

plt.figure(figsize=(16,6))

sns.scatterplot(x=dados['bmi'], y=dados['charges'])
plt.title("IMC x Taxas")
plt.show()
```

![IMC x Taxas 1.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/IMC_x_Taxas_1.png)

Clicando na imagem, podemos fazer uma primeira an√°lise preeliminar, nela vemos que existe uma tend√™ncia do pagamento de taxas mais altas para qual o aumento do IMC, ent√£o podemos considerar uma correla√ß√£o positiva. Podemos enxergar melhor tra√ßando uma linha nesse gr√°fico

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\insurance.csv')
print(dados.head())

plt.figure(figsize=(16,6))

sns.regplot(x=dados['bmi'], y=dados['charges'])
plt.title("IMC x Taxas")
plt.show()
```

![IMC x Taxas 2.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/IMC_x_Taxas_2.png)

Dentro desse tipo de modelo, podemos segmentar um pouco mais, como foi dito acima, existe uma coluna para pessoas que fumam ou n√£o fumam, afinal, ser√° que existe alguma influ√™ncia? Visualmente conseguimos fazer essa an√°lise

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\insurance.csv')
print(dados.head())

plt.figure(figsize=(16,6))

sns.scatterplot(x=dados['bmi'], y=dados['charges'], hue=dados["smoker"])
plt.title("IMC x Taxas X Fumantes")
plt.show()
```

Antes de continuarmos, usamos um novo fator na constru√ß√£o do gr√°fico, o ‚ÄúHue‚Äù. Ele vem diretamente do ingl√™s, em tradu√ß√£o literal pode ser algo relacionado a tom/tonalidade, por isso √© usado para colora√ß√£o desses pontos que estaremos apresentando no gr√°fico abaixo

![IMC x Taxas 3.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/IMC_x_Taxas_3.png)

Focando na an√°lise, j√° sab√≠amos que existia uma correla√ß√£o entre o aumento do IMC e quanto as pessoas pagam pelos seguros, agora vemos uma clara rela√ß√£o n√£o apenas entre as vari√°veis anteriores mas com os fumantes, eles claramente a medida que o IMC aumenta, come√ßam a pagar valores ainda maiores  

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\insurance.csv')
print(dados.head())

plt.figure(figsize=(16,6))

sns.lmplot(x='bmi', y='charges', hue='smoker', data=dados)
plt.title("IMC x Taxas X Fumantes")
plt.show()
```

![IMC x Taxas 4.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/IMC_x_Taxas_4.png)

Esse gr√°fico foi usada uma outra fun√ß√£o mas a ideia continua sendo a mesma, a principal diferen√ßa √© que est√°vamos selecionando dentro das vari√°veis que comp√µe o gr√°fico suas informa√ß√µes, puxando elas de sua vari√°vel original diretamente para visualiza√ß√£o

Nessa a √∫nica diferen√ßa √© que precisamos deixar claro para fun√ß√£o de onde ela estar√° puxando as informa√ß√µes, pois n√£o teremos como fazer no mesmo formato dos anteriores

Isso explicado, vamos a uma pequena an√°lise do que nos foi printado, como foi explicado, ficou clara a correla√ß√£o dos fumantes e n√£o fumantes, agora para mostra-la de forma mais clara, inserimos 2 linhas, uma para cada categoria

### **Fun√ß√µes SNS dos gr√°ficos**

- **Trends**- Uma tend√™ncia √© definida como um padr√£o de mudan√ßa.
    - `sns.lineplot` - **Gr√°ficos de linha** s√£o melhores para mostrar tend√™ncias ao longo do tempo, e v√°rias linhas podem ser usadas para mostrar tend√™ncias em mais de um grupo.
- **Relationship**¬†- Existem muitos tipos diferentes de gr√°ficos que voc√™ pode usar para entender as rela√ß√µes entre vari√°veis em seus dados.
    - `sns.barplot` - **Gr√°ficos de barras** s√£o √∫teis para comparar quantidades correspondentes a diferentes grupos.
    - `sns.heatmap` - **Mapas de calor** podem ser usados para encontrar padr√µes codificados por cores em tabelas de n√∫meros.
    - `sns.scatterplot` - **Gr√°ficos de dispers√£o** mostram a rela√ß√£o entre duas vari√°veis cont√≠nuas; se codificados por cores, tamb√©m podemos mostrar a rela√ß√£o com uma terceira [vari√°vel categ√≥rica](https://en.wikipedia.org/wiki/Categorical_variable).
    - `sns.regplot` - Incluir uma **linha de regress√£o** no gr√°fico de dispers√£o facilita a visualiza√ß√£o de qualquer rela√ß√£o linear entre duas vari√°veis.
    - `sns.lmplot` - Este comando √© √∫til para desenhar v√°rias linhas de regress√£o, se o gr√°fico de dispers√£o contiver m√∫ltiplos grupos codificados por cores.
    - `sns.swarmplot` - **Gr√°ficos de dispers√£o categ√≥ricos** mostram a rela√ß√£o entre uma vari√°vel cont√≠nua e uma vari√°vel categ√≥rica.
- **Distribution**¬†- Visualizamos distribui√ß√µes para mostrar os valores poss√≠veis que podemos esperar ver em uma vari√°vel, juntamente com a probabilidade deles ocorrerem.
    - `sns.histplot` - **Histogramas** mostram a distribui√ß√£o de uma √∫nica vari√°vel num√©rica.
    - `sns.kdeplot` - **Gr√°ficos KDE** (ou **gr√°ficos KDE 2D**) mostram uma distribui√ß√£o estimada e suavizada de uma √∫nica vari√°vel num√©rica (ou duas vari√°veis num√©ricas).
    - `sns.jointplot` - Este comando √© √∫til para exibir simultaneamente um gr√°fico KDE 2D com os gr√°ficos KDE correspondentes para cada vari√°vel individual.
    
    Vale lembrar disso aqui: Esta fun√ß√£o abaixo altera os estilos (cor de fundo)
    
    ```python
    # Alterar o estilo da figura para o tema "dark"
    sns.set_style("dark")
    
    ```
    
    E os temas dispon√≠veis s√£o:
    
    - darkgrid
    - whitegrid
    - dark
    - white
    - ticks
    
    **Exemplo**: Tema normal
    
    ![](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Figura_tema_normal.png)
    
    **Exemplo**: Tema escuro
    
    ![](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Figura_tema_dark.png)
    

## **Tratamento de dados**

### **Data Cleaning**

- Arquivos

Vamos trabalhar aqui um b√°sico para tratamento de dados, principalmente dados vazios.

Basicamente, vamos tratar de 2 formas, substituindo os valores vazios e, digamos, os ‚Äúabandonando‚Äù, retirando eles do dataframe

A primeira forma, extremamente vi√°vel para termos uma no√ß√£o de como os dados est√£o, √© usar uma soma.

Como assim? Bom, podemos simplesmente fazer a contagem, tanto de todo o dataframe quanto de uma coluna especifica. Ent√£o, vamos l√° demonstrar isso üôÇ

```python
from cProfile import label
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\NFL Play by Play 2009-2018 (v5).csv')
print(dados.head())

var=dados.isnull().sum()
print(var[0:10])
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2016.png)

O c√≥digo acima, como pode ver esta pegando o arquivo e somando o n√∫mero de c√©lulas brancas, nesse caso, selecionamos as 11 primeiras colunas e vemos que s√£o valores consideraveis

No exemplo abaixo, buscamos somar todas as porcentagens das colunas relacionados aos valores vazios e trazer os dados de suas porcentagens

```python
import pandas as pd
import numpy as np

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\NFL Play by Play 2009-2018 (v5).csv')
print(dados.head())

var=dados.isnull().sum()

total=np.product(dados.shape)
por=(var/total)*100
print(sum(por))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2017.png)

Vemos que o total √© de 38,5% de dados vazios, mais de 1/3.

Bom, e como podemos remover esses dados vazios?

Temos 2 formas, substituindo esses dados ou dando drop.

Vamos para a alternativa do Drop

```python
import pandas as pd
import numpy as np

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\NFL Play by Play 2009-2018 (v5).csv')
print(dados.head())

print(dados.dropna())
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2018.png)

Como podemos ver, o resultado foi um dataframe vazio. O motivo √© simples, o dropna() da drop em todas as linhas com algum NaN, ou seja, todas as linhas tinham pelo menos um NaN em alguma coluna, uma alternativa interessante √© inverter e inv√©s de utilizarmos as linhas, vamos usar as colunas usando axis=1

```python
import pandas as pd
import numpy as np

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\NFL Play by Play 2009-2018 (v5).csv')
print(dados.head())

print(dados.dropna(axis=1))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2019.png)

Agora, para substituirmos dados, usamos o fillna() e dentro do parenteses usamos o valor que desejamos que o python substitua 

No exemplo abaixo, demos print em uma coluna que possuia um NA e substituimos esse valor por zero

```python
import pandas as pd
import numpy as np

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\NFL Play by Play 2009-2018 (v5).csv')
print(dados.head())
print(dados['posteam'])

dados=dados.fillna(0)
print(dados['posteam'])
```

DataFrame original, sem o fillna()

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2020.png)

DataFrame com o fillna(0)

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2021.png)

### **Trabalhando com datas**

- **Arquivos**

Bom, quando carregamos dados dentro do Python, n√£o √© incomum termos alguma coluna com datas, e n√£o datas no sentido de dados (em ingl√™s data) mas sim realmente datas, como dd/mm/yy ou em outros formatos.

Por√©m n√≥s sabermos que tal coluna s√£o datas quer dizer que o Python vai conseguir identificar da mesma forma? 

A resposta para essa pergunta √© n√£o, ent√£o vamos precisar fazer esse apontamento a ele. Vejamos no exemplo abaixo:

```python
import pandas as pd
import numpy as np
import seaborn as sns
import datetime

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\catalog.csv')

np.random.seed(0)

print(dados.iloc[:7,:4])
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2022.png)

Aqui estamos printando as 7 primeiras linhas e 4 primeiras colunas mas a √∫nica que nos interessa √© a coluna date, vamos tentar identificar como o Python est√° identificando essa coluna, e isso podemos fazer de dois jeitos, usando .head() e .dtype. As duas formas estar√£o abaixo

```python
import pandas as pd
import numpy as np
import seaborn as sns
import datetime

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\catalog.csv')

np.random.seed(0)

print(dados['date'].head())

print(dados['date'].dtype)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2023.png)

O .head() est√° indo at√© a linha escrito ‚ÄúName:date, dtype:object‚Äù 

A linha logo abaixo √© a do dtype, ambas elas funcionam para aquilo que precisamos. 

Agora, bom notarmos, temos o resultado do tipo de dado, no caso, ‚Äúobject‚Äù. Ent√£o para trabalharmos com datas, devemos converter.

```python
import pandas as pd
import numpy as np
import seaborn as sns
import datetime

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\catalog.csv')

np.random.seed(0)

dados['date']=pd.to_datetime(dados['date'])

print(dados['date'].head())
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2024.png)

Agora, conseguimos ver que o Python identifica a coluna date como uma coluna com datas.

### **Entradas de dados inconsistentes**

- Arquivos
    
    [pakistan_intellectual_capital.csv](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/pakistan_intellectual_capital.csv)
    

Como sabemos, ao estarmos usando uma base de dados, podemos ter inconsist√™ncias. Nesse caso em especifico, me refiro a inconsist√™ncias como nomes sem letras mai√∫sculas ou com e sem espa√ßo, para isso existem formas de tratar esses dados.

Ent√£o, vamos come√ßar com o c√≥digo abaixo:

```python
import pandas as pd
import numpy as np
import fuzzywuzzy
from fuzzywuzzy import process
import chardet

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\pakistan_intellectual_capital.csv')

np.random.seed(0)

print(dados.head())
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2025.png)

No caso desse arquivo, vamos trabalhar com a coluna ‚ÄúCountry‚Äù, vamos tentar padronizar os nomes e entender se pode ter acontecido digita√ß√µes erradas do nome dos pa√≠ses. Ent√£o, vamos l√°

```python
import pandas as pd
import numpy as np
import fuzzywuzzy
from fuzzywuzzy import process
import chardet

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\pakistan_intellectual_capital.csv')

np.random.seed(0)

Country=dados['Country']
Country=Country.unique()
print(Country)
print(sorted(Country))
```

Nesse c√≥digo, estamos puxando a coluna Country e usando a fun√ß√£o Unique para termos apenas dados n√£o repetidos, ap√≥s isso os deixamos em ordem alfab√©tica. O primeiro print abaixo, servir√° para mostrar a fun√ß√£o unique, o segundo o sorted(ordem alfab√©tica) e o terceiro os dois juntos no compilador do Python junto com algumas explica√ß√µes

Fun√ß√£o Unique

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2026.png)

Fun√ß√£o sorted

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2027.png)

Ambos juntos:

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2028.png)

Bom, agora que temos a vis√£o completa tem algo legal para percebermos, apesar de estar em ordem alfab√©tica, Germany, New Zealand, Swaden e USA est√£o antes de Australia. O motivo disso √© apenas um, espa√ßos em branco antes da primeira letra, enquanto tamb√©m temos germany em √∫ltimo, depois de Urbana, o motivo √© que n√£o tem letra mai√∫scula 

Como podemos tratar isso? Bom, podemos deixar tudo maisculo ou minusculo e remover os espa√ßos em branco

Vamos fazer os dois casos mas vou continuar com min√∫sculo (Simplesmente pq eu quero üôÇ)

```python
import pandas as pd
import numpy as np
import fuzzywuzzy
from fuzzywuzzy import process
import chardet

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\pakistan_intellectual_capital.csv')

np.random.seed(0)

dados['Country']=dados['Country'].str.upper()
print(dados['Country'])

dados['Country']=dados['Country'].str.lower()
print(dados['Country'])

dados['Country']=dados['Country'].str.strip()

Country=dados['Country']
Country=Country.unique()
print(Country)
print(sorted(Country))
```

Podemos ver aqui a diferen√ßa de aplica√ß√£o entre mai√∫sculas e min√∫sculas com .upper (mai√∫sculas) e .lower(min√∫sculas).

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2029.png)

Na imagem abaixo, conseguimos ver melhor a aplica√ß√£o da fun√ß√£o .lower junto da unique(), com a fun√ß√£o strip() que remove os espa√ßos em branco tanto do come√ßo de uma string quanto do final

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2030.png)

E por fim, sorted()

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2031.png)

Como podemos ver, os problemas anteriormente registrados, foram corrigidos!!

Mas ainda temos uma outra situa√ß√£o. ‚ÄúSouth korea‚Äù e ‚ÄúSouthKorea‚Äù. Claro, poderiamos substituir usando pandas como vimos acima em outros m√≥dulos, mas podemos fazer de uma forma um pouco mais complexa e que permitir√° fazer mais de uma vez usando uma fun√ß√£o

Nesse cen√°rio, √© de f√°cil an√°lise pq n√£o s√£o muitos dados e s√£o apenas de paises, por isso a id√©ia da fun√ß√£o √© recomendada, ent√£o vamos come√ßar a an√°lise 

A fuzzywyzzy ajuda a entender strings que sejam pr√≥ximas uma da outra com um certo tipo de rela√ß√£o, ajudando a poupar algum tempo

```python
import pandas as pd
import numpy as np
import fuzzywuzzy
from fuzzywuzzy import process
import chardet

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\pakistan_intellectual_capital.csv')

np.random.seed(0)

dados['Country']=dados['Country'].str.lower()
dados['Country']=dados['Country'].str.strip()

Country=dados['Country']
Country=Country.unique()

matches = fuzzywuzzy.process.extract("south korea", Country, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)
print(matches)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2032.png)

Essa √© a sa√≠da, 100 √© o valor m√°ximo e a medida que vai diminuindo, encontramos strings sem nenhuma rela√ß√£o, com rela√ß√£o a southkorea com 48. Podemos partir ent√£o de que o valor minimo dessaa rela√ß√£o √© 47

Ent√£o, vamos criar uma fun√ß√£o que fa√ßa todo o trabalho acima e mais do que isso, tamb√©m nos permita substituir southkorea por south korea.

```python
import pandas as pd
import numpy as np
import fuzzywuzzy
from fuzzywuzzy import process
import chardet

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\pakistan_intellectual_capital.csv')

np.random.seed(0)

def replace_matches(df,column,string_to_match,min_ratio=47):

    #Ajustando padr√£o dos dados para minusculas e sem espa√ßos em brancos no come√ßo e final
    df[column]=df[column].str.lower()
    df[column]=df[column].str.strip()
    
    #criando um data frame para usar o fuzzywuzzy
    strings=df[column].unique()

    #Cria a lista que mostrei no c√≥digo anterior
    matches=fuzzywuzzy.process.extract(string_to_match, strings, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

    #Adiciona em uma v√°riavel os matches, os resultados do fuzzywuzzy pr√≥ximo do valor pr√©-definido
    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]

    #Seleciona as linhas com esses matches
    rows_with_matches = df[column].isin(close_matches)

    #Substitui southkorea por shouth korea
    df.loc[rows_with_matches, column] = string_to_match

    print("Pronto")

replace_matches(df=dados,column='Country',string_to_match='south korea')

country=dados['Country'].unique()
print(sorted(country))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2033.png)

## **Machine Learning - Int**

### **Decision Tree**

- Arquivos
    
    [melb_data.csv](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/melb_data.csv)
    

Bom, vamos tentar trazer, primeiramente, uma ideia do que √© Machine Learning. Basicamente, √© um m√©todo de an√°lise de dados que busca automatizar modelos anal√≠ticos, uma ideia de que as m√°quinas podem aprender com o algoritmo, sendo basicamente uma vertente da intelig√™ncia artificial.

Aqui, n√£o chegaremos t√£o longe quanto intelig√™ncia artifical mas veremos um modelo para usarmos previs√£o e o modelo que usaremos se chama ‚ÄúDecision Tree‚Äù.

Basicamente, teremos diversas variaveis e baseadas nelas, quando tivermos x tipos de informa√ß√£o, o modelo conseguir√° prever o que desejamos

Ex visual de uma ‚ÄúDecision Tree‚Äù:

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2034.png)

Bom, nos arquivos que enviei podemos ver que s√£o relacionados a casas, pode ser de uma imobili√°ria, de um corretor, um especulador, enfim. Com isso temos uma id√©ia, devemos tentar prever o pre√ßo de casas de acordo com algumas vari√°veis, ent√£o vamos l√°!

Nesse caso, estamos apenas mostrando a ideia de um primeiro modelo, n√£o vamos trabalhar esses dados de forma profunda

```python
import pandas as pd

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\melb_data.csv')

dados=dados.dropna()

print(dados)
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2035.png)

Vamos definir o que queremos prever, ou seja, o pre√ßo. Ela dever√° ser armazenada na vari√°vel Y enquanto os demais itens que v√£o compor a previs√£o, aqueles que podem ter algum tipo de relev√¢ncia para o que estamos fazendo, deve estar presente na vari√°vel X

```python
import pandas as pd
from sklearn.tree import DecisionTreeRegressor

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\melb_data.csv')

dados=dados.dropna()

print(dados)

y=dados['Price']

x=pd.concat((dados['Rooms'],dados['Bathroom'],dados['Landsize'],dados['Lattitude'],dados['Longtitude']),axis=1)

print(x)
print(y)

Arv=DecisionTreeRegressor(random_state=1)
Arv.fit(x,y)
print(Arv)

print("previs√£o de 5 casas")
print(x.head())
print('Previs√µes:')
print(Arv.predict(x.head()))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2036.png)

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2037.png)

Bom, na realidade esse primeiro modelo tem a fun√ß√£o de mostrar como estruturar o modelo em s√≠. Ainda n√£o estamos prevendo mas agora j√° temos uma pequena base em como fazer isso

- **Define:**¬†What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.
- **Fit:**¬†Capture patterns from provided data. This is the heart of modeling.
- **Predict:**¬†Just what it sounds like
- **Evaluate**: Determine how accurate the model's predictions are.

### **Valida√ß√£o de modelo**

Quase todos os modelos de previs√£o que for montado, vamos querer que ele seja preciso (√≥bvio), a medida relevante da qualidade do modelo √© a precis√£o preditiva, basicamente, se as previs√µes do modelo v√£o estar pr√≥ximas do que realmente acontece.

Um erro comum, importante de ser mencionado, √© que muitas pessoas fazem a valida√ß√£o usando os valores que estamos buscando, √© confuso, mas logo mais vai ficar mais claro

A v√°rias formas de fazer essa mensura√ß√£o, uma delas √© usando a M√©dia do Erro Absuluto, ou Mean Absolute Errot (MAE)

```python
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\melb_data.csv')

dados=dados.dropna()

print(dados)

y=dados['Price']

x=pd.concat((dados['Rooms'],dados['Bathroom'],dados['Landsize'],dados['BuildingArea'],dados['YearBuilt'],dados['Lattitude'],dados['Longtitude']),axis=1)

print(x)
print(y)

arv=DecisionTreeRegressor()
arv.fit(x,y)

#######Calculo do Mean Absolute Error

previsao=arv.predict(x)
print(previsao)
print(mean_absolute_error(y,previsao))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2038.png)

Bom, por esse teste temos um modelo de grande qualidade, afinal, o erro est√° em algo pr√≥ximo de apenas 500 d√≥lares. Esse tipo de modelo √© chamado de modelo ‚ÄúIn-Sample‚Äù, por√©m temos um problema!

Usamos uma √∫nica amostra de dados para avaliar esse modelo, uma √∫nica amostra tanto para construir o modelo quanto para avali√°-lo

Imagine que, no grande mercado imobili√°rio, a cor da porta n√£o tem rela√ß√£o com o pre√ßo da casa.

No entanto, na amostra de dados que voc√™ usou para construir o modelo, todas as casas com portas verdes eram muito caras. O trabalho do modelo √© encontrar padr√µes que prevejam os pre√ßos das casas, para que ele veja esse padr√£o e sempre preveja pre√ßos altos para casas com portas verdes.

Como esse padr√£o foi derivado dos dados de treinamento, o modelo parecer√° preciso nos dados de treinamento.

Mas se esse padr√£o n√£o se mantiver quando o modelo vir novos dados, o modelo ser√° muito impreciso quando usado na pr√°tica.

Como o valor pr√°tico dos modelos vem de fazer previs√µes em novos dados, medimos o desempenho em dados que n√£o foram usados para construir o modelo. A maneira mais direta de fazer isso √© excluir alguns dados do processo de constru√ß√£o do modelo e us√°-los para testar a precis√£o do modelo em dados que ele n√£o viu antes. Esses dados s√£o chamados de dados de valida√ß√£o.

Para verificarmos isso, vamos usar uma fun√ß√£o da sklern.model_selection chamada train_test_split. A fun√ß√£o dela √© quebrar os dados em 2 peda√ßos e uma parte ser√° usada para treinar o modelo e a outra para calcular a MAE

```python
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\melb_data.csv')

dados=dados.dropna()

print(dados)

y=dados['Price']

x=pd.concat((dados['Rooms'],dados['Bathroom'],dados['Landsize'],dados['BuildingArea'],dados['YearBuilt'],dados['Lattitude'],dados['Longtitude']),axis=1)

print(x)
print(y)

train_x,val_x,train_y,val_y=train_test_split(x,y, random_state=0)

arv=DecisionTreeRegressor()
arv.fit(train_x,train_y)

#######Calculo do Mean Absolute Error

previsao=arv.predict(val_x)
print(previsao)
print(mean_absolute_error(val_y,previsao))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2039.png)

Pois bem, esse tipo de modelo que fizemos agora se chama modelo ‚ÄúOut-of-Sample‚Äù, vemos agora o valor no final e conseguimos perceber que o modelo n√£o estava t√£o correto assim, ou seja, no modelo in-sample o erro era de 500 d√≥lares enquanto agora √© mais de 25000, uma grande diferen√ßa j√° que trabalhamos os dados e incluimos de forma que servisse como um novo dado para validar o modelo.

Ent√£o, legal comentar, uma casa de um milh√£o de d√≥lares teria uma taxa de erro de 250000, ou seja, quase um quarto do seu valor

### **Over e Under**

Basicamente, overfitting e underfitting s√£o contr√°rios um do outro, se voc√™ souber ingl√™s, j√° deve ter uma m√≠nima ideia do que isso quer dizer

**Overfitting**

Imagine que voc√™ montou seu banco de dados e seu modelo de machine learning, a medida que a arvore vai se aprofundando, mais ‚Äúfolhas‚Äù s√£o geradas. Bom, isso teoricamente √© bom pq as caracter√≠sticas das casas seriam extremamente abrangidas e os valores ficariam pr√≥ximos da realidade pelos treinos mas, quando novos dados fossem entrar, a valida√ß√£o seria pobre pois ter√≠amos muitos poucos valores dentro de cada folha

**Underfitting**

Pelo lado contr√°rio, se dividirmos em poucos grupos, ter√≠amos uma grande n√∫mero de casas dentro de cada grupo, mas, por termos poucos grupos, possivelmente n√£o ter√≠amos como estimar os valores corretamente pois muitas caracteristicas seriam ignoradas pelo modelo

### **Floresta Aleat√≥ria/Random Forest**

A ideia de √°rvore vem diretamente das Decision Trees, afinal, se tivermos uma √°rvore com muitas ou com poucas folhas, podemos ter resultados que n√£o ser√£o interessantes para n√≥s, como j√° explicado mais acima.

Bom, o que s√£o as Random Forests? Bom, ela usamuitas √°rvores e faz a previs√£o calculando a m√©dia de previs√£o de cada √°rvore, o que geralmente leva a ter modelos melhores

```python
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

dados=pd.read_csv('C:\\Users\\lucas\\Downloads\\melb_data.csv')

dados=dados.dropna()

print(dados)

y=dados['Price']

x=pd.concat((dados['Rooms'],dados['Bathroom'],dados['Landsize'],dados['BuildingArea'],dados['YearBuilt'],dados['Lattitude'],dados['Longtitude']),axis=1)

print(x)
print(y)

train_x,val_x,train_y,val_y=train_test_split(x,y, random_state=0)

arv=DecisionTreeRegressor()
arv.fit(train_x,train_y)

previsao=arv.predict(val_x)
print(previsao)
print(mean_absolute_error(val_y,previsao))

#Floresta aleat√≥ria

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_x, train_y)
melb_preds = forest_model.predict(val_x)
print(mean_absolute_error(val_y, melb_preds))
```

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2040.png)

![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2041.png)

Bom, podemos ver que o resultado do modelo anterior era de 261724 aproximadamente, usando as √°rvores, temos um erro menor, agora sendo de aproximadamente 191700, uma grande evolu√ß√£o

## **Deep Learning**

### **O que √© Deep Learning**

Alguns dos avan√ßos mais impressionantes em intelig√™ncia artificial nos √∫ltimos anos foram no campo do aprendizado profundo. Tradu√ß√£o de linguagem natural, reconhecimento de imagem e jogabilidade s√£o tarefas em que os modelos de aprendizado profundo se aproximaram ou at√© superaram o desempenho em n√≠vel humano.

Ent√£o, o que √© aprendizado profundo? O aprendizado profundo √© uma abordagem de aprendizado de m√°quina caracterizada por pilhas profundas de c√°lculos. Essa profundidade de computa√ß√£o √© o que permitiu que os modelos de aprendizado profundo separassem os tipos de padr√µes complexos e hier√°rquicos encontrados nos conjuntos de dados do mundo real mais desafiadores.

Por meio de seu poder e escalabilidade, as redes neurais se tornaram o modelo definidor de aprendizado profundo. As redes neurais s√£o compostas por neur√¥nios, onde cada neur√¥nio realiza individualmente apenas uma simples computa√ß√£o. O poder de uma rede neural vem da complexidade das conex√µes que esses neur√¥nios podem formar.

- **Unidade Linear**
    
    ![Untitled](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%2042.png)
    
    *Unidade Linear: y = wx+b*
    
    Bom, unidade linear √© bem parecido com equa√ß√µes de primeiro grau. A entrada dos dados √© X e elas possuem um peso W, ou seja, o que seja ao neur√¥nio (nome dessa figura) √© w*x. A rede neural ‚Äúaprende‚Äù modificando seus pesos
    
    O B √© um tipo de peso especial chamado vi√©s, ele n√£o possui nenhum dado de entrada, ent√£o colocamos 1 e multiplicamos pelo b, confuso? Sim, mas tente se orientar pela imagem, acho que √© possivel ficar mais claro.Por fim, o y √© o valor que o neur√¥nio finalmente produz
    

## **Dash - Plotly**

### **B√°sico**

- **O que √© Dash py?**
    
    Dash √© uma biblioteca de python, o foco dela √©, como podemos imaginar pelo nome, criar visuais interativos para os dados com os quais voc√™ estiver trabalhando. 
    
    Ela busca fazer isso em um servidor offline, √© interessante a analise dessa lib, pois √© uma grande facilitadora. Caso n√£o tivessemos ela, seria necess√°rio a utiliza√ß√£o de alguma lib para criar uma estrutura e de outra para cria√ß√£o de um servidor, por fim combinar com java.
    
- **Componentes e instala√ß√£o**
    
    Dash possui 3 bibliotecas distintas: dash (que como j¬¥a citamos ¬¥e uma extensÀúao do Servidor Web), core (seus elementos principais) e html (como o nome diz s√£o as tags HTML que utilizaremos para
    estruturar nosso Dashboard).
    
    ```python
    import dash
    import dash_core_components as dcc
    import dash_html_components as html
    ```
    
    - Primeiro programa
        
        Bom, tendo isso em mente, vamos ver como seria um primeiro programa de Dash
        
- **Layout e tratamentos de dados**
- **Callbacks simples**

### **Intermedi√°rio**

- **Aplica√ß√£o de gr√°ficos**
    - Tipos de gr√°ficos
    - Componentes dos gr√°ficos
- **Callbacks avan√ßados e Dropdown**
- **Mais sobre layout e Div‚Äôs (Bootstrap)**

## **Engenharia de Dados - Sua jornada**

## **GitHub, Poetry e PIPX**

### PIPX:

pip √© do pr√≥prio python, √© uma ferramenta bem est√°vel mas que as novidades demoram para chegar no pip

```python
$ pipx install NOME DO PACOTE
```

### Git:

Apagar todas as pastas:

```powershell
pip freeze | Where-Object { $_ -notmatch "^-e" } | Set-Content packages.txt
pip uninstall -y (Get-Content packages.txt)
Remove-Item packages.txt

```

Criar pasta:

```jsx
$ mkdir Nome_Pasta
```

Acessar pasta:

```jsx
$ cd Nome_Pasta
```

Instalar Vers√£o do python

```jsx
$ pyenv install x.xx.x
```

Criar ambiente Virtual:

```python
$ python -m venv Nome_Ambiente

#por padr√£o:#

$ python -m venv .venv
```

Para ativar o ambiente virtual:

```python
$ source .venv/Scripts/activate
```

para Desativar:

```python
$ deactivate
```

para voltar a pasta anterior:

```python
$ cd ..
```

Para remover um diret√≥rio:

```python
$ rm -r Nome_diret√≥rio
```

### Poetry:

Logo na tela inicial do Bash:

```python
lucas.brecht@LFNTBSPITP115 MINGW64
$ poetry config virtualenvs.in-project true
```

Para criar a pasta:

```python
$ poetry new nome pasta
```

Definindo a vers√£o python

```python
$ pyenv local x.xx.x
```

Dizendo ao poetry qual vers√£o usar:

```python
$ poetry env use x.xx.x
#Ele cria automaticamente a pasta do venv ap√≥s esse comando
```

E para instalar pacotes:

```python
$ poetry add xxx
```

## **Docker**

## CHAT GPT

## üì¶ O que √© Docker?

**Docker** √© uma plataforma que permite criar, implantar e executar aplica√ß√µes em **containers** ‚Äî ambientes isolados que empacotam uma aplica√ß√£o com todas as suas depend√™ncias, bibliotecas e configura√ß√µes.

## ‚úÖ Vantagens do Docker

- Portabilidade (roda em qualquer sistema com Docker instalado)
- Facilidade de versionamento de ambientes
- Consist√™ncia entre ambientes de desenvolvimento, teste e produ√ß√£o
- Isolamento de aplica√ß√µes
- Automatiza√ß√£o com Dockerfiles e Compose

## üöÄ Como instalar o Docker

## Windows

1. Baixe o **Docker Desktop**:
    
    üëâ https://www.docker.com/products/docker-desktop
    
2. Instale e reinicie o sistema, se necess√°rio.
3. Habilite a **virtualiza√ß√£o** no BIOS, se for solicitado.
4. Docker CLI pode ser acessado via terminal (PowerShell, CMD, ou WSL).

## macOS

1. Baixe o Docker Desktop para Mac:
    
    üëâ https://www.docker.com/products/docker-desktop
    
2. Instale normalmente (arraste o √≠cone para ‚ÄúAplica√ß√µes‚Äù).
3. Pode usar via terminal (Terminal.app ou iTerm).

## Linux (Ubuntu/Debian)

```bash
bash
CopiarEditar
sudo apt update
sudo apt install -y \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

# Adiciona chave GPG
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \
  sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

# Adiciona o reposit√≥rio
echo \
  "deb [arch=$(dpkg --print-architecture) \
  signed-by=/etc/apt/keyrings/docker.gpg] \
  https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Instala o Docker Engine
sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Habilita uso do Docker sem sudo
sudo usermod -aG docker $USER
newgrp docker

```

## ‚öôÔ∏è Principais comandos do Docker

### Containers

```bash
bash
CopiarEditar
docker run hello-world                      # Roda container de teste
docker run -it ubuntu bash                 # Abre shell bash em Ubuntu
docker ps                                  # Lista containers em execu√ß√£o
docker ps -a                               # Lista todos os containers
docker stop <container_id>                # Para container
docker rm <container_id>                  # Remove container

```

### Imagens

```bash
bash
CopiarEditar
docker images                              # Lista imagens locais
docker pull python:3.10                    # Baixa imagem do Python
docker rmi <image_id>                      # Remove imagem

```

### Dockerfile

Crie um arquivo `Dockerfile`:

```
Dockerfile
CopiarEditar
FROM python:3.10

WORKDIR /app

COPY requirements.txt .

RUN pip install -r requirements.txt

COPY . .

CMD ["python", "main.py"]

```

### Build e run com Dockerfile

```bash
bash
CopiarEditar
docker build -t minha-app .
docker run minha-app

```

### Volumes (para persist√™ncia)

```bash
bash
CopiarEditar
docker run -v /meu/diretorio:/app -it python bash

```

## üß∞ Docker Compose

Docker Compose √© uma ferramenta para orquestrar m√∫ltiplos containers com um arquivo YAML.

### Exemplo: `docker-compose.yml`

```yaml
yaml
CopiarEditar
version: '3.8'

services:
  web:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - .:/app
    depends_on:
      - redis

  redis:
    image: redis:alpine

```

Comandos:

```bash
bash
CopiarEditar
docker-compose up        # Sobe os containers
docker-compose down      # Derruba os containers

```

## üêç Integra√ß√£o com Python

## 1. Criando containers para rodar scripts Python

Com um Dockerfile como este:

```
Dockerfile
CopiarEditar
FROM python:3.10-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

CMD ["python", "script.py"]

```

Voc√™ consegue rodar:

```bash
bash
CopiarEditar
docker build -t meu-script-python .
docker run meu-script-python

```

## 2. Usando bibliotecas Python para se comunicar com o Docker

### Usando `docker-py`

```bash
bash
CopiarEditar
pip install docker

```

```python
python
CopiarEditar
import docker

client = docker.from_env()

# Lista containers
for container in client.containers.list(all=True):
    print(container.name, container.status)

# Roda container com imagem Python
container = client.containers.run("python:3.10", "echo hello from container", remove=True)

```

## 3. Criando APIs Python (FastAPI, Flask) e empacotando com Docker

```
Dockerfile
CopiarEditar
FROM python:3.10

WORKDIR /app
COPY . .

RUN pip install fastapi uvicorn

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

```

## 4. Com Jupyter Notebook

Rodar Jupyter dentro do container:

```bash
bash
CopiarEditar
docker run -p 8888:8888 -v $PWD:/app -w /app jupyter/base-notebook

```

Ou criar seu pr√≥prio Dockerfile com o Jupyter:

```
Dockerfile
CopiarEditar
FROM python:3.10

RUN pip install notebook pandas matplotlib

WORKDIR /app
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--allow-root"]

```

## üß† Boas pr√°ticas

- Use `.dockerignore` (evita copiar arquivos desnecess√°rios)
- Crie imagens leves (use `python:slim`)
- Sempre fixe as vers√µes de depend√™ncias (`requirements.txt`)
- Use **multi-stage builds** para imagens menores
- Use volumes nomeados para persist√™ncia de dados

## üìö Exemplos de uso no mundo real

- Criar ambientes para rodar ETLs Python
- Empacotar notebooks para ci√™ncia de dados
- Criar APIs para an√°lises em produ√ß√£o
- Automatizar jobs com containers
- Integra√ß√£o cont√≠nua (CI/CD)
- Orquestra√ß√£o com Kubernetes (avan√ßado)

## Alguns c√≥digos

```powershell
# Exemplo 1: Script Python simples em Docker
# Estrutura:
# .
# ‚îú‚îÄ‚îÄ Dockerfile
# ‚îú‚îÄ‚îÄ requirements.txt
# ‚îî‚îÄ‚îÄ script.py

# script.py
print("Hello from Docker + Python!")

# requirements.txt
# (vazio, j√° que esse exemplo n√£o usa libs externas)

# Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "script.py"]

# Exemplo 2: API com FastAPI + Docker
# Estrutura:
# .
# ‚îú‚îÄ‚îÄ app
# ‚îÇ   ‚îî‚îÄ‚îÄ main.py
# ‚îú‚îÄ‚îÄ requirements.txt
# ‚îî‚îÄ‚îÄ Dockerfile

# app/main.py
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def root():
    return {"message": "Hello from FastAPI + Docker!"}

# requirements.txt
fastapi
uvicorn

# Dockerfile
FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY ./app ./app
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# Exemplo 3: Python + Pandas + SQLite
# Estrutura:
# .
# ‚îú‚îÄ‚îÄ app.py
# ‚îú‚îÄ‚îÄ requirements.txt
# ‚îî‚îÄ‚îÄ Dockerfile

# app.py
import pandas as pd
import sqlite3

conn = sqlite3.connect('data.db')
df = pd.DataFrame({"nome": ["Ana", "Jo√£o"], "idade": [28, 34]})
df.to_sql('pessoas', conn, if_exists='replace', index=False)

resultado = pd.read_sql('SELECT * FROM pessoas', conn)
print(resultado)

# requirements.txt
pandas

# Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "app.py"]

# Exemplo 4: Jupyter Notebook com Docker
# Comando para rodar:
# docker run -p 8888:8888 -v "$PWD":/app -w /app jupyter/base-notebook

# Ou criando seu pr√≥prio Dockerfile:
# Dockerfile
FROM python:3.10
RUN pip install notebook pandas matplotlib
WORKDIR /app
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--allow-root"]

# Exemplo 5: Docker Compose com FastAPI + Redis
# Estrutura:
# .
# ‚îú‚îÄ‚îÄ docker-compose.yml
# ‚îú‚îÄ‚îÄ app
# ‚îÇ   ‚îî‚îÄ‚îÄ main.py
# ‚îî‚îÄ‚îÄ requirements.txt

# app/main.py
from fastapi import FastAPI
import redis

app = FastAPI()
r = redis.Redis(host='redis', port=6379)

@app.get("/")
def read_root():
    r.incr('visitas')
    return {"visitas": int(r.get('visitas'))}

# requirements.txt
fastapi
uvicorn
redis

# Dockerfile
FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY ./app ./app
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# docker-compose.yml
version: '3.8'
services:
  web:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    depends_on:
      - redis

  redis:
    image: redis:alpine

```

## Curso

A id√©ia do Docker √© ser uma virtualiza√ß√£o, o Docker surgiu como um complemento a uma maquina virtual, √© basicamente, utilizar o computador da melhor forma poss√≠vel.

![image.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/image.png)

Imagina que temos um Servidor rodando 40 aplica√ß√µes

Na VM teria que instalar 40 linux, enquanto no Docker voc√™ vai ter a engine Docker com o Linux e vai espelhar para todos eles.

![image.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/image%201.png)

Docker j√° da alguns tutoriais, ent√£o j√° come√ßa com: O que √© um container?

Dentro do Container, na aba exec, ele √© um ‚ÄúBash‚Äù, funciona da mesma forma e com a mesma lingagem, ls, mkdis, cd, etc, etc‚Ä¶

O Docker hub tem varias imagens e programas prontos, funciona de forma meio parecida que o Git Hub, ele tem o docker pull, docker build, docker run, etc, etc

![image.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/image%202.png)

Ok vamos l√°:
No python, elaborei esse c√≥digo:

```python
import streamlit as st

def hello_world():
    return "Hello, World!"

def main():
    st.write(hello_world())

if __name__ == '__main__':
    main()
```

Apenas lembrando, estou usando o poetry no 3.11.3, preciso comee√ßar a fazer git pushs

Agora, se uma outra pessoa fosse utilizar o meu arquivo, ela conseguiria? 

Bom, sim, pelo git, ela faria um git clone do meu resposit√≥rio, em seguida, teria que seguir uma sequ√™ncia de passos, coomo pip install pipx, pipx install poetry, poetry add isso, poetry tal coisa aquilo

Bom, ser√° que nenhuma alma caridosa pensou em uma forma de automatizar isso? Pois bem, obviamente que sim

Seguindo o passo a passo da imagem acima (A que tem DockerFile, Code, Image e Container), o c√≥digo acima √© simplesmente, o Code.

No Dockerfile, podemos configurar para que ele puxe TUDO isso que usamos para desenvolver essa aplica√ß√£o, com essa linha abaixo:

```docker
FROM python:3.11.3
RUN pip install poetry
COPY . /src
WORKDIR /src
RUN poetry install 
EXPOSE 8501
ENTRYPOINT [ "poetry", "run", "streamlit", "run", "app/app.py", "--server.port=8501", "--server.aaddress=0.0.0.0" ]
```

No pr√≥prio Docker, temos como se fosse uma ‚Äúbiblioteca‚Äù que puxa o python e a vers√£o que est√° usando, quando ele for executado, automaticamente vai dar o pip install necess√°rio (desde que voc√™ defina). 

O terceiro, o Copy, ele copia todo o conte√∫do da pasta e joga na pasta source do Docker que estamos criando, √© como se fosse um linux j√° com uma pasta source, com todo o c√≥digo + o python, funcionando da melhor forma poss√≠vel

O WorkDir n√£o entendi muito bem, mas o c√≥digo geralmente vai abrir na pasta raiz do Docker, configurando dessa forma, ele vai abrir na pasta source (n√£o sei o pq isso √© bom ou ruim)

Run poetry install para ele instalar as bibliotecas do poetry

O Docker, o container vai fechar para todos, vai retirar do mundo a execu√ß√£o do meu c√≥digo, configurando o EXPOSE 8501, ele vai permitir que consiga acessar essa aplica√ß√£o pela porta 8501

A √∫ltima linha, basicamente, estou dizendo pra ele executar essa linha de c√≥digo do python:

```python
poetry run streamlit run app.py
```

Dizendo qual porta vai ser acessada e qual o endere√ßo do servidor, como √© o servidor local, ent√£o √© o 0.0.0.0

Agora vamos buildar a imagem, entenda o build como se fosse um Zip

```docker
docker build -t minha-primeira-imagem
```

Com isso, a imagem √© criada. Ela √© meio pesada, nessa execu√ß√£o ele vai bauxar todo o python, todo o poetry, as bibliotecas junto ao projeto, etc,etc,etc

Com isso tendo sido rodado, ele j√° vai aparecer no Docker Desktop

![image.png](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/image%203.png)

Ok, recebemos a imagem, mas com haviamos falado, a imagem √© como se fosse um Zip, como isso vira um container? 

Executando esse c√≥digo:

```docker
$ docker run -d -p 8501:8501 --name meu-primeiro-container minha-primeira-imagem

```

-d significa deatachment, basicamente, pra ele rodar em segundo plano pra n√£o prender o terminal

-p significa porta, meio que √© a forma dele conversar com a aplica√ß√£o, o streamlit configuramos pra 8501 ser a porta pra acessarmos ele, no docker image dizemos qual porta precisa ficar aberta para o mundo, aqui √© quando abrimos ela

‚Äîname definimos o nome do container

Em seguida passamos o nome da imagem

Para criar um Docker Compose, que nada mais √© do que v√°rias imagens de docker para criar um container, ou seja, mais de uma imagem por container

No exemplo abaixo, temos um caso para criarmos um Docker Compose com o Postgres e Pgadmin

```docker
version: '3'

services:
  teste-postgres-compose:
    image: postgres
    environment:
      POSTGRES_PASSWORD: "Postgres2019!"
    ports:
      - "15432:5432"
    volumes:
      - /home/renatogroffe/Desenvolvimento/Docker-Compose/PostgreSQL:/var/lib/postgresql/data 
    networks:
      - postgres-compose-network
      
  teste-pgadmin-compose:
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: "renatogroff@yahoo.com.br"
      PGADMIN_DEFAULT_PASSWORD: "PgAdmin2019!"
    ports:
      - "16543:80"
    depends_on:
      - teste-postgres-compose
    networks:
      - postgres-compose-network

networks: 
  postgres-compose-network:
    driver: bridge
```

## **Airflow**

O Apache Airflow √© uma plataforma open-source que permite criar, agendar e monitorar workflows de dados programaticamente. √â amplamente utilizado para orquestrar processos complexos de ETL (Extract, Transform, Load), pipelines de dados e outras opera√ß√µes automatizadas.

- **1.** **Documenta√ß√£o Oficial**
    
    A documenta√ß√£o do Apache Airflow √© extensa e cobre diversos aspectos, incluindo:
    
    - **Instala√ß√£o e Configura√ß√£o**: Como instalar o Airflow em diferentes ambientes (local, Docker, Kubernetes) e configurar as depend√™ncias.
    - **Conceitos B√°sicos**: Introdu√ß√£o aos principais conceitos como DAGs (Directed Acyclic Graphs), Tasks, Operadores, e Schedulers.
    - **Refer√™ncia de API**: Detalhamento das fun√ß√µes, classes e m√≥dulos dispon√≠veis para uso.
    - **Tutoriais e Exemplos**: Passo a passo para criar seus primeiros workflows e exemplos de casos de uso comuns.
    - **Guia de Desenvolvimento**: Como contribuir para o projeto, estender funcionalidades e trabalhar com o c√≥digo-fonte do Airflow.
    
    Voc√™ pode acessar a documenta√ß√£o oficial [aqui](https://airflow.apache.org/docs/).
    
- **2.** **Funcionalidades Principais**
    - **DAGs (Directed Acyclic Graphs)**: A estrutura central do Airflow, representando workflows. Cada DAG √© composto por uma cole√ß√£o de tarefas organizadas em uma sequ√™ncia espec√≠fica.
    - **Operadores**: Componentes que executam tarefas espec√≠ficas dentro dos DAGs. Existem operadores para v√°rias fun√ß√µes, como execu√ß√£o de scripts Python, opera√ß√µes em bancos de dados, intera√ß√µes com APIs, etc.
    - **Tarefas e Depend√™ncias**: As tarefas dentro de um DAG podem ser dependentes umas das outras. O Airflow garante que as depend√™ncias sejam respeitadas e executadas na ordem correta.
    - **Scheduler**: O componente respons√°vel por monitorar os DAGs e acionar a execu√ß√£o das tarefas conforme programado.
    - **UI Web**: Uma interface web amig√°vel para visualizar DAGs, monitorar a execu√ß√£o de tarefas, ver logs e depurar erros.
    - **Plugins e Extens√µes**: O Airflow √© altamente extens√≠vel, permitindo a adi√ß√£o de operadores personalizados, sensores, hooks, etc.
    - **Logs e Monitoramento**: Logs detalhados e recursos de monitoramento integrados para acompanhar o estado de execu√ß√£o dos workflows.
- **3. Formas de Uso**
    - **Cria√ß√£o de DAGs**: Voc√™ pode criar DAGs utilizando Python. O c√≥digo define as tarefas e as rela√ß√µes de depend√™ncia entre elas.
    - **Agendamento de Workflows**: Workflows podem ser agendados para serem executados em intervalos regulares, como diariamente, semanalmente, ou em hor√°rios espec√≠ficos.
    - **Integra√ß√£o com Outros Sistemas**: O Airflow pode se integrar com v√°rios sistemas, como bancos de dados, servi√ßos de nuvem (AWS, GCP), ferramentas de Big Data (Hadoop, Spark), e muito mais.
    - **Execu√ß√£o Local e em Produ√ß√£o**: O Airflow pode ser executado tanto em ambiente local para desenvolvimento quanto em clusters distribu√≠dos para processamento em larga escala.
- **Exemplos de Casos de Uso**
    - **Pipeline de ETL**: Extra√ß√£o de dados de um banco de dados, transforma√ß√£o em um formato adequado, e carregamento em um data warehouse.
    - **Orquestra√ß√£o de Processos de Machine Learning**: Automa√ß√£o de fluxos de trabalho de treinamento, valida√ß√£o e implanta√ß√£o de modelos de ML.
    - **Automa√ß√£o de Processos de Neg√≥cio**: Agendamento e execu√ß√£o de processos cr√≠ticos de neg√≥cios, como gera√ß√£o de relat√≥rios ou sincroniza√ß√£o de dados entre sistemas.
- **Exemplos Pr√°ticos**
    
    Vou te fornecer alguns exemplos pr√°ticos em Python para criar e agendar workflows (DAGs) no Apache Airflow. Isso inclui a cria√ß√£o de uma DAG simples, a defini√ß√£o de tarefas, operadores b√°sicos e a configura√ß√£o de depend√™ncias.
    
    ### 1. **Instala√ß√£o do Airflow**
    
    Antes de tudo, voc√™ precisa instalar o Apache Airflow. Aqui est√° o comando para instalar o Airflow utilizando o pip:
    
    ```bash
    pip install apache-airflow
    
    ```
    
    ### 2. **Exemplo 1: Criando uma DAG Simples**
    
    Neste exemplo, vamos criar uma DAG simples que executa tr√™s tarefas sequenciais: a primeira imprime uma mensagem, a segunda dorme por 5 segundos e a terceira imprime outra mensagem.
    
    ```python
    from airflow import DAG
    from airflow.operators.python_operator import PythonOperator
    from datetime import datetime, timedelta
    
    # Fun√ß√µes Python que ser√£o executadas como tarefas
    def print_start():
        print("Iniciando o processo...")
    
    def sleep_for_a_while():
        import time
        time.sleep(5)
    
    def print_end():
        print("Processo conclu√≠do!")
    
    # Configura√ß√µes padr√£o da DAG
    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': datetime(2024, 8, 20),
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    }
    
    # Definindo a DAG
    with DAG(
        'exemplo_dag_simples',
        default_args=default_args,
        description='Uma DAG simples com tr√™s tarefas',
        schedule_interval=timedelta(days=1),
        catchup=False
    ) as dag:
    
        # Tarefas
        start_task = PythonOperator(
            task_id='print_start',
            python_callable=print_start,
        )
    
        sleep_task = PythonOperator(
            task_id='sleep_for_a_while',
            python_callable=sleep_for_a_while,
        )
    
        end_task = PythonOperator(
            task_id='print_end',
            python_callable=print_end,
        )
    
        # Definindo a sequ√™ncia de execu√ß√£o
        start_task >> sleep_task >> end_task
    
    ```
    
    ### 3. **Exemplo 2: Utilizando BashOperator**
    
    Agora, vamos criar uma DAG que utiliza o `BashOperator` para executar comandos do shell.
    
    ```python
    from airflow import DAG
    from airflow.operators.bash_operator import BashOperator
    from datetime import datetime, timedelta
    
    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': datetime(2024, 8, 20),
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    }
    
    with DAG(
        'exemplo_bash_operator',
        default_args=default_args,
        description='Uma DAG que usa BashOperator',
        schedule_interval=timedelta(days=1),
        catchup=False
    ) as dag:
    
        # Tarefa que imprime a data atual
        print_date_task = BashOperator(
            task_id='print_date',
            bash_command='date',
        )
    
        # Tarefa que dorme por 10 segundos
        sleep_task = BashOperator(
            task_id='sleep',
            bash_command='sleep 10',
        )
    
        # Tarefa que imprime "Airflow √© legal!"
        echo_task = BashOperator(
            task_id='echo',
            bash_command='echo "Airflow √© legal!"',
        )
    
        # Definindo a sequ√™ncia de execu√ß√£o
        print_date_task >> sleep_task >> echo_task
    
    ```
    
    ### 4. **Exemplo 3: Transfer√™ncia de Dados Entre Tarefas**
    
    Neste exemplo, vamos transferir dados de uma tarefa para outra utilizando o `XComs` do Airflow. A primeira tarefa gera um valor e a segunda tarefa o utiliza.
    
    ```python
    from airflow import DAG
    from airflow.operators.python_operator import PythonOperator
    from datetime import datetime, timedelta
    
    def generate_data(**kwargs):
        value = 42
        kwargs['ti'].xcom_push(key='my_value', value=value)
    
    def consume_data(**kwargs):
        ti = kwargs['ti']
        value = ti.xcom_pull(key='my_value', task_ids='generate_data_task')
        print(f"O valor recebido foi: {value}")
    
    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': datetime(2024, 8, 20),
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    }
    
    with DAG(
        'exemplo_xcoms',
        default_args=default_args,
        description='Uma DAG que usa XComs para passar dados entre tarefas',
        schedule_interval=timedelta(days=1),
        catchup=False
    ) as dag:
    
        generate_data_task = PythonOperator(
            task_id='generate_data_task',
            python_callable=generate_data,
            provide_context=True,
        )
    
        consume_data_task = PythonOperator(
            task_id='consume_data_task',
            python_callable=consume_data,
            provide_context=True,
        )
    
        generate_data_task >> consume_data_task
    
    ```
    
    ### Como Executar
    
    1. Salve os c√≥digos acima em arquivos `.py` dentro da pasta de DAGs configurada no Airflow (geralmente `~/airflow/dags`).
    2. Acesse a interface web do Airflow (`http://localhost:8080` por padr√£o).
    3. Ative e execute as DAGs criadas. Voc√™ ver√° as tarefas sendo executadas conforme programado.
    
    Esses exemplos fornecem uma boa base para entender como criar DAGs no Airflow. Se precisar de mais detalhes ou de exemplos mais complexos, estou √† disposi√ß√£o!
    

## **Rest/Fast API**

## **Kafka**

## **Data WareHouse**

## **Streamlit**

# SQL

# PostGreSQL Documenta√ß√£o

[https://www.notion.so](https://www.notion.so)

# C√≥digos auxiliares

## Ordem Exexu√ß√£o

![Fqtt03faAAAdyYp.jpeg](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Fqtt03faAAAdyYp.jpeg)

## Duvidas Aleat√≥rias que surgiram durante a Vida

### Qual a diferen√ßa de usar With e Sub-Query?

O que o eu, Lucas entendi no momento que surgiu a d√∫vida:

√â que, me parece, que sub-query possui uma leitura mais complicada, o c√≥digo me parece mais desorganizado, em um teste, o desempenho com o With ficou melhor. Percebi tambpem que consigo referenciar o With outras vezes, coisa que n√£o consigo na Subquery

O que meu amigo robozinho entende:

### **`WITH` (Common Table Expressions - CTE) vs. Subquery no PostgreSQL**

Tanto **`WITH` (CTE)** quanto **subqueries** permitem criar consultas intermedi√°rias para organizar o c√≥digo SQL. No entanto, existem diferen√ßas de desempenho e legibilidade entre elas.

---

## üìå **1. O que √© uma Subquery?**

Uma **subquery (subconsulta)** √© uma consulta aninhada dentro de outra. Ela pode ser usada em `SELECT`, `FROM` ou `WHERE`.

### **üîπ Exemplo de Subquery no `FROM`**

```sql
SELECT ranked.*
FROM (
    SELECT
        produtos.product_id AS "C√≥digo do Produto",
        produtos.product_name AS "Nome do Produto",
        SUM(detalhes.quantity) AS "Quantidade Total Vendida",
        RANK() OVER (ORDER BY SUM(detalhes.quantity) DESC) AS "Ranking"
    FROM public.order_details detalhes
    LEFT JOIN public.products produtos
        ON detalhes.product_id = produtos.product_id
    GROUP BY produtos.product_id, produtos.product_name
) ranked
WHERE ranked."Ranking" <= 5;

```

‚úî **Vantagens:**

- N√£o precisa de nome global para a subquery.
- Pode ser usada dentro de `SELECT`, `FROM` ou `WHERE`.

‚ùå **Desvantagens:**

- Pode ficar dif√≠cil de ler quando h√° muitas subqueries aninhadas.
- Em alguns casos, pode ser **menos eficiente** do que `WITH`, pois a subquery pode ser recalculada v√°rias vezes.

---

## üìå **2. O que √© `WITH` (CTE - Common Table Expression)?**

O `WITH` cria uma **tabela tempor√°ria nomeada** que pode ser reutilizada na consulta principal.

### **üîπ Exemplo de `WITH` (CTE)**

```sql
WITH ranked AS (
    SELECT
        produtos.product_id AS "C√≥digo do Produto",
        produtos.product_name AS "Nome do Produto",
        SUM(detalhes.quantity) AS "Quantidade Total Vendida",
        RANK() OVER (ORDER BY SUM(detalhes.quantity) DESC) AS "Ranking"
    FROM public.order_details detalhes
    LEFT JOIN public.products produtos
        ON detalhes.product_id = produtos.product_id
    GROUP BY produtos.product_id, produtos.product_name
)
SELECT * FROM ranked WHERE "Ranking" <= 5;

```

‚úî **Vantagens:**

- **C√≥digo mais leg√≠vel**, pois voc√™ pode nomear e reutilizar a CTE.
- **Reutiliza√ß√£o**: Se precisar da mesma consulta em diferentes partes do c√≥digo, basta referenciar a CTE.
- **Melhor otimiza√ß√£o pelo PostgreSQL**, pois a CTE pode ser materializada (armazenada temporariamente) antes de ser usada.

‚ùå **Desvantagens:**

- Se a CTE **n√£o for inline**, pode consumir mais mem√≥ria.

---

## üìå **3. Quando usar `WITH` (CTE) ou Subquery?**

| Situa√ß√£o | Usar `WITH` (CTE) | Usar Subquery |
| --- | --- | --- |
| C√≥digo mais organizado e reutiliz√°vel | ‚úÖ Sim | ‚ùå N√£o |
| Melhor desempenho ao evitar recalcula√ß√µes | ‚úÖ Sim | ‚ùå Pode ser ruim |
| Simples e r√°pida, sem necessidade de reutiliza√ß√£o | ‚ùå Pode ser exagero | ‚úÖ Sim |
| PostgreSQL precisa otimizar a execu√ß√£o | ‚úÖ Sim | ‚ùå Pode repetir c√°lculos |

---

### **üìå Resumo Final**

- Use **`WITH` (CTE)** quando precisar de **c√≥digo organizado e reutiliz√°vel**.
- Use **subquery** para **consultas simples** que n√£o precisam ser reutilizadas.
- Para **grandes conjuntos de dados**, **testar o desempenho** pode ser importante para escolher a melhor abordagem.

üöÄ Agora voc√™ pode decidir qual usar no seu SQL! Se precisar de mais detalhes, me avise. üòä

Comentario Final: Me parece que estava certo :)

## Verificar informa√ß√µes das tabelas

```sql
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_name = 'zzz';
```

## Verificar primeira linha

```sql
SELECT * FROM schema.table 
LIMIT 2;
```

## Utiliza√ß√£o de IF (No caso, em SQL √© o Case)

```sql
SELECT id, 
       nome, 
       salario,
       CASE 
           WHEN salario > 5000 THEN 'Alto'
           WHEN salario BETWEEN 3000 AND 5000 THEN 'M√©dio'
           ELSE 'Baixo'
       END AS categoria_salario
FROM funcionarios;

```

## Utiliza√ß√£o do Having

```sql
SELECT detalhes.product_id AS "C√≥digo do Produto",
       SUM(detalhes.quantity) AS "Quantidade Total Vendida"
FROM public.order_details detalhes
GROUP BY detalhes.product_id
HAVING SUM(detalhes.quantity) > 100;
```

## Utiliza√ß√£o do RANK()

```sql
SELECT 
    produtos.category_id AS "C√≥digo da Categoria",
    produtos.product_id AS "C√≥digo do Produto",
    produtos.product_name AS "Nome do Produto",
    SUM(detalhes.quantity) AS "Quantidade Total Vendida",
    ROUND(SUM(detalhes.quantity * detalhes.unit_price)::NUMERIC, 2) AS "Total",
    RANK() OVER (
        PARTITION BY produtos.category_id 
        ORDER BY SUM(detalhes.quantity * detalhes.unit_price) DESC
    ) AS "Ranking"
FROM public.order_details detalhes
LEFT JOIN public.products produtos 
    ON detalhes.product_id = produtos.product_id
GROUP BY produtos.category_id, produtos.product_id, produtos.product_name;

```

## Utiliza√ß√£o de Fun√ß√µes de Janela para substituir Group By

```sql
SELECT DISTINCT
    produtos.category_id AS "C√≥digo da Categoria",
    produtos.product_id AS "C√≥digo do Produto",
    produtos.product_name AS "Nome do Produto",
    SUM(detalhes.quantity * detalhes.unit_price) OVER (
        PARTITION BY produtos.category_id, produtos.product_id, produtos.product_name
    ) AS "Total"
FROM public.order_details detalhes
LEFT JOIN public.products produtos 
    ON detalhes.product_id = produtos.product_id;
```

No exemplo abaixo, as duas querys est√£o fazendo a mesma coisa, o resultado √© o mesmo, me fica como sugest√£o testar a efici√™ncia com um conjunto de dados mais pesado

```sql
SELECT 
    pedidos.order_id AS "Numero do Pedido",
    detalhes.product_id AS "C√≥digo do Produto",
    pedidos.order_date AS "Data do Pedido",
    pedidos.freight AS "Peso Pedido",
    pedidos.ship_city AS "Cidade Destino",
    detalhes.quantity AS "Quantidade",
    detalhes.unit_price AS "Pre√ßo Unit√°rio",
    ROUND((detalhes.quantity * detalhes.unit_price)::NUMERIC, 2) AS "Total"
FROM public.orders pedidos
LEFT JOIN public.order_details detalhes 
    ON pedidos.order_id = detalhes.order_id;

SELECT DISTINCT
    produtos.category_id AS "C√≥digo da Categoria",
    produtos.product_id AS "C√≥digo do Produto",
    produtos.product_name AS "Nome do Produto",
	COUNT (*) OVER (
        PARTITION BY produtos.category_id, produtos.product_id, produtos.product_name
    ) AS "Numero de Vendas",
    SUM(detalhes.quantity * detalhes.unit_price) OVER (
        PARTITION BY produtos.category_id, produtos.product_id, produtos.product_name
    ) AS "Total"
FROM public.order_details detalhes
LEFT JOIN public.products produtos 
    ON detalhes.product_id = produtos.product_id
WHERE 
	produtos.category_id = 7;

SELECT DISTINCT
    produtos.category_id AS "C√≥digo da Categoria",
	COUNT (*) OVER (
        PARTITION BY produtos.category_id
    ) AS "Numero de Vendas",
    SUM(detalhes.quantity * detalhes.unit_price) OVER (
        PARTITION BY produtos.category_id
    ) AS "Total"
FROM public.order_details detalhes
LEFT JOIN public.products produtos 
    ON detalhes.product_id = produtos.product_id
WHERE 
	produtos.category_id = 7;

SELECT
    produtos.category_id AS "C√≥digo da Categoria",
	COUNT (*) AS "Numero de Vendas",
    SUM(detalhes.quantity * detalhes.unit_price) "Total"
FROM public.order_details detalhes
LEFT JOIN public.products produtos 
    ON detalhes.product_id = produtos.product_id
WHERE 
	produtos.category_id = 7
GROUP BY
    produtos.category_id;
```

Tentei urilizar uma fun√ß√£o Janela junto com o RANK, mas n√£o sei se o PostDegrees n√£o aceita ou se √© algo em comum no SQL, inclusive em outros bancos. Vou colocar a id√©ia aqui para poder testar em no Banco Oracle ou SQLServer da Luft

```sql
SELECT DISTINCT
    produtos.category_id AS "C√≥digo da Categoria",
    produtos.product_id AS "C√≥digo do Produto",
    produtos.product_name AS "Nome do Produto",
    SUM(detalhes.quantity * detalhes.unit_price) OVER (
        PARTITION BY produtos.category_id, produtos.product_id, produtos.product_name
    ) AS "Total",
    RANK() OVER (
        PARTITION BY produtos.category_id 
        ORDER BY SUM(detalhes.quantity * detalhes.unit_price) OVER (
            PARTITION BY produtos.category_id, produtos.product_id, produtos.product_name
        ) DESC
    ) AS "Ranking"
FROM public.order_details detalhes
LEFT JOIN public.products produtos 
    ON detalhes.product_id = produtos.product_id;
```

# **Lean**

Anota√ß√µes que fiz enquanto fazia a trilha

[view?usp=drivesdk](https://drive.google.com/file/d/1GuiKmSYpoQ6kT8hSQOjJEpXPcwh2H7eW/view?usp=drivesdk)

# **Livros**

- **Data Science do Zero: Primeiras Regras com Python**
    
    [Data Science do Zero - Primeiras Regras com Python.pdf](https://drive.google.com/file/d/1HEMwWnn_qD18-3avsmlKaJIJEm-yxN_o/view?usp=drivesdk)
    
- **R for Data Science (Material em Ingl√™s)**
    
    [R for Data Science.pdf](https://drive.google.com/file/d/1BgMrYIQ-uRlkpEnPmjQ4h3OL6G9IEzK9/view?usp=drivesdk)
    
- **Estat√≠stica Pratica para Cientistas de Dados**
    
    [Estatistica Pratica para Cientistas de Dados.pdf](https://drive.google.com/file/d/1ojhaaqa8zU2nEjpCJuaQXNcwbPfp9_gQ/view?usp=drivesdk)
    
- **Gest√£o de For√ßa de Vendas**
    
    [view?usp=share_link](https://drive.google.com/file/d/1BE6zxZhYeeFVowjWKXmzrX2yF4tqzWRP/view?usp=share_link)
    
- **Administra√ß√£o de Vendas**
    
    [ADMINISTRA√á√ÉO DE VENDAS - 2¬™ edi√ß√£o.pdf](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/ADMINISTRAO_DE_VENDAS_-_2_edio.pdf)
    

[](Escolinha%20%F0%9F%AB%95%203ff711aeae3f4f1593cf2585e5e5b498/Untitled%20502ca13295624315b91df25e5a097695.md)